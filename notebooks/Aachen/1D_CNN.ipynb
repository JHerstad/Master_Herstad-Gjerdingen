{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/2122214546.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsAAAAGJCAYAAACEkIXWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABNJElEQVR4nO3deXxN1/7/8fchc0iCyGQMUfNUQ0TNXEFQVyeqai4tVfSqum1N1erVGlrVqippi6vVgdZYooZqaKVSs4vSoBKzGGJK1u+P/rK/jgSJhqT26/l47Ed79v7svdc6g7yzs/Y6DmOMEQAAAGAT+XK7AQAAAMDdRAAGAACArRCAAQAAYCsEYAAAANgKARgAAAC2QgAGAACArRCAAQAAYCsEYAAAANgKARgAAAC2QgAG8qhRo0bJ4XDclXM1adJETZo0sR6vXr1aDodDX3zxxV05f/fu3VW6dOm7cq7bde7cOfXu3VtBQUFyOBwaNGhQjhz3br7Of8X17xEg3d/h8wtcjwAM3AXR0dFyOBzW4uHhoZCQEEVGRuqdd97R2bNnc+Q8f/zxh0aNGqX4+PgcOV5Oystty4rXX39d0dHRevrpp/Xpp5+qa9euN6wtXbp0hte7XLlyGjp0qE6ePHkXW333Xd93b29v1a1bV5988kmG2vTPxaZNmzI9Vtu2bTMEK4fDoQEDBtyyHZ9//rnq1asnPz8/FSlSRI0bN9bixYsz1KWlpWn8+PEKDQ2Vh4eHqlWrpv/+979Z6+z/Fx8fryeeeEIlSpSQu7u7ChcurBYtWmjWrFlKTU3N1rGkP99rCxYsyPZ+ALLOJbcbANjJmDFjFBoaqitXrigxMVGrV6/WoEGDNHHiRH3zzTeqVq2aVfvyyy/rxRdfzNbx//jjD40ePVqlS5dWjRo1srzfd999l63z3I6bte3DDz9UWlraHW/DX7Fq1SrVq1dPI0eOzFJ9jRo19Pzzz0uSLl68qLi4OE2ePFlr1qzRTz/9ZNXdzuuc113b9yNHjmjGjBnq1q2bLl26pD59+tzx80+ZMkUDBw5UVFSU3njjDV28eFHR0dFq27atvvzyS3Xs2NGqfemll/TGG2+oT58+qlOnjhYuXKjHH39cDodDnTp1uuW5ZsyYoX79+ikwMFBdu3ZVuXLldPbsWcXExKhXr146cuSI/v3vf2er/a+//roefvhhdejQIbtdzxV/h88vcD0CMHAXtW7dWrVr17YeDx8+XKtWrVLbtm3Vvn177dy5U56enpIkFxcXubjc2Y/ohQsX5OXlJTc3tzt6nltxdXXN1fNnxdGjR1WpUqUs1xcrVkxPPPGE9bh3794qUKCA3nrrLe3Zs0flypWTdHde57vt+r53795dZcqU0aRJk+5aAK5Tp46+/fZba3hJz549VaxYMX388cdWAD58+LAmTJig/v37691335X05+vUuHFjDR06VI888ojy589/w/Ns2LBB/fr1U0REhJYsWaKCBQta2wYNGqRNmzZp27Ztd7Cnuev8+fPy9vb+W3x+gesxBALIZc2aNdMrr7yi33//XbNnz7bWZzY2dMWKFWrQoIH8/PxUoEABlS9f3rq6tHr1atWpU0eS1KNHD+tP0NHR0ZL+HMNZpUoVxcXFqVGjRvLy8rL2vdH4ztTUVP373/9WUFCQvL291b59ex08eNCppnTp0urevXuGfa895q3altkYwvPnz+v555+3/qxcvnx5vfXWWzLGONWl/0l8wYIFqlKlitzd3VW5cmUtW7Ys8yf8OkePHlWvXr0UGBgoDw8PVa9eXR9//LG1PX089P79+7V48WKr7QcOHMjS8a8VFBQkSU6BN7PXOat9St9379696t69u/z8/OTr66sePXrowoULGc4/e/Zs1apVS56enipcuLA6deqU4fWUpOnTp6ts2bLy9PRU3bp1tW7dumz39VpFixZVhQoVtG/fvr90nKxKTk5WQECA0/Pq4+OjAgUKWL9gStLChQt15coVPfPMM9Y6h8Ohp59+WocOHVJsbOxNzzN69Gg5HA7NmTPHKfymq127ttNn46233lL9+vVVpEgReXp6qlatWhnG2TscDp0/f14ff/yx9V679hiHDx9Wz549FRgYaL0vZs6cmeHcv//+u9q3by9vb28FBARo8ODBWr58uRwOh1avXu1UO3/+fOt94e/vryeeeEKHDx92qunevbsKFCigffv2qU2bNipYsKC6dOlibbv+85uWlqbJkyercuXK8vDwUGBgoPr27atTp0451W3atEmRkZHy9/eXp6enQkND1bNnzxs95UCOubcuOwB/U127dtW///1vfffddze8QrZ9+3a1bdtW1apV05gxY+Tu7q69e/dq/fr1kqSKFStqzJgxGjFihJ566ik1bNhQklS/fn3rGCdOnFDr1q3VqVMnPfHEEwoMDLxpu1577TU5HA4NGzZMR48e1eTJk9WiRQvFx8c7BYlbyUrbrmWMUfv27fX999+rV69eqlGjhpYvX66hQ4fq8OHDmjRpklP9Dz/8oK+++krPPPOMChYsqHfeeUcPPfSQEhISVKRIkRu2KyUlRU2aNNHevXs1YMAAhYaGav78+erevbtOnz6t5557ThUrVtSnn36qwYMHq3jx4taf9osWLXrTPl+5ckXHjx+X9OcQiM2bN2vixIlq1KiRQkNDb/mcZadPjz76qEJDQzVu3Dj98ssvmjFjhgICAvSf//zHqnnttdf0yiuv6NFHH1Xv3r117NgxTZkyRY0aNdLmzZvl5+cnSfroo4/Ut29f1a9fX4MGDdJvv/2m9u3bq3DhwipRosQt252Zq1ev6tChQypUqNBt7Z9dTZo00RdffKEpU6aoXbt2unjxoqZMmaIzZ87oueees+o2b94sb29vVaxY0Wn/unXrWtsbNGiQ6TkuXLigmJgYNWrUSCVLlsxSu95++221b99eXbp00eXLlzVv3jw98sgjWrRokaKioiRJn376qXr37q26devqqaeekiSVLVtWkpSUlKR69epZvyAVLVpUS5cuVa9evZScnGzdmHn+/Hk1a9ZMR44c0XPPPaegoCDNnTtX33//fYY2RUdHq0ePHqpTp47GjRunpKQkvf3221q/fr3T+0L683WMjIxUgwYN9NZbb8nLy+uGfe3bt6917IEDB2r//v169913tXnzZq1fv16urq46evSoWrZsqaJFi+rFF1+Un5+fDhw4oK+++ipLzyfwlxgAd9ysWbOMJPPzzz/fsMbX19fUrFnTejxy5Ehz7Ud00qRJRpI5duzYDY/x888/G0lm1qxZGbY1btzYSDLTpk3LdFvjxo2tx99//72RZIoVK2aSk5Ot9Z9//rmRZN5++21rXalSpUy3bt1uecybta1bt26mVKlS1uMFCxYYSWbs2LFOdQ8//LBxOBxm79691jpJxs3NzWndr7/+aiSZKVOmZDjXtSZPnmwkmdmzZ1vrLl++bCIiIkyBAgWc+l6qVCkTFRV10+NdWyspw/LAAw+Y48ePO9Ve/zpnp0/p+/bs2dNp/3/+85+mSJEi1uMDBw6Y/Pnzm9dee82pbuvWrcbFxcVaf/nyZRMQEGBq1KhhLl26ZNVNnz7dSHJ6PW/W95YtW5pjx46ZY8eOma1bt5quXbsaSaZ///5Otbf6XERFRTm9L9Kfm+uPc72kpCTTvHlzp+fe39/f/PjjjxmOX6ZMmQz7nz9/3kgyL7744g3Pkf56PPfcczdty7UuXLjg9Pjy5cumSpUqplmzZk7rvb29M/1M9erVywQHB2d4D3Xq1Mn4+vpax58wYYKRZBYsWGDVpKSkmAoVKhhJ5vvvv7fOHxAQYKpUqWJSUlKs2kWLFhlJZsSIEda6bt263fA5uf7zu27dOiPJzJkzx6lu2bJlTuu//vrrW/67CNwpDIEA8ogCBQrcdDaI9CsxCxcuvO0bTtzd3dWjR48s1z/55JNOf9p9+OGHFRwcrCVLltzW+bNqyZIlyp8/vwYOHOi0/vnnn5cxRkuXLnVa36JFC+sqmSRVq1ZNPj4++u233255nqCgIHXu3Nla5+rqqoEDB+rcuXNas2bNbfchPDxcK1as0IoVK7Ro0SK99tpr2r59u9q3b6+UlJRb7p+dPvXr18/pccOGDXXixAklJydLkr766iulpaXp0Ucf1fHjx60lKChI5cqVs64Mbtq0SUePHlW/fv2cxoV3795dvr6+We77d999p6JFi6po0aKqWrWqPv30U/Xo0UNvvvlmlo/xV3h5eal8+fLq1q2b5s+fr5kzZyo4OFgdO3bU3r17rbqUlBS5u7tn2N/Dw8PafiPpz21mQx9u5Nq/mpw6dUpnzpxRw4YN9csvv9xyX2OMvvzyS7Vr107GGKfXMTIyUmfOnLGOs2zZMhUrVkzt27d36tP1f11Kf72feeYZq8+SFBUVpQoVKmQ6a8bTTz99y7bOnz9fvr6++sc//uHUzlq1aqlAgQLW+y3937RFixbpypUrtzwukJMYAgHkEefOnVNAQMANtz/22GOaMWOGevfurRdffFHNmzdXx44d9fDDDytfvqz9LlusWLFs3fCWfqNWOofDobCwsNsa/5odv//+u0JCQjKEi/Q/Vf/+++9O6zP7E3ShQoUyjDfM7DzlypXL8Pzd6DzZ4e/vrxYtWliPo6KiVL58eT388MOaMWOGnn322Zvun50+XV+bPtTg1KlT8vHx0Z49e2SMyfB6pku/iSm9v9fXubq6qkyZMjdt77XCw8M1duxYpaamatu2bRo7dqxOnTp1Wzdb3s4cyY888ohcXFz07bffWusefPBBlStXTi+99JI+++wzSX8G0kuXLmXY/+LFi9b2G/Hx8ZGkbE1huGjRIo0dO1bx8fFO581KH48dO6bTp09r+vTpmj59eqY1R48elfTn61i2bNkMxw0LC3N6nP56ly9fPsOxKlSooB9++MFpnYuLi4oXL37Ltu7Zs0dnzpy54b9n6e1s3LixHnroIY0ePVqTJk1SkyZN1KFDBz3++OOZ/mIC5CQCMJAHHDp0SGfOnMnwA+panp6eWrt2rb7//nstXrxYy5Yt02effaZmzZrpu+++u+nd6tceI6fd6Id3ampqltqUE250HnPdDXO5rXnz5pKktWvX3jIAZ6dPt6pNS0uTw+HQ0qVLM60tUKDATduSXdeG/8jISFWoUEFt27bV22+/rSFDhlh1t7rSeuHCBacrk1nx22+/admyZRlCYuHChdWgQQNrzLwkBQcH6/vvv5cxxul9fOTIEUlSSEjIDc8TFhYmFxcXbd26NUvtWrdundq3b69GjRrpvffeU3BwsFxdXTVr1izNnTv3lvun/9XniSeeULdu3TKtuXYaxTvB3d09S79sp6WlKSAgQHPmzMl0e/r4+fQv29mwYYO+/fZbLV++XD179tSECRO0YcOGHH9fAtciAAN5wKeffirpz7BwM/ny5VPz5s3VvHlzTZw4Ua+//rpeeuklff/992rRokWOf6PYnj17nB4bY7R3716nH7SFChXS6dOnM+z7+++/O101zE7bSpUqpZUrV+rs2bNOV4F37dplbc8JpUqV0pYtW5SWlub0gz2nz5Pu6tWrkv682n83lS1bVsYYhYaG6r777rthXXp/9+zZo2bNmlnrr1y5ov3796t69eq3df6oqCg1btxYr7/+uvr27Stvb2+n8+3evdu6MfJa//vf/1SlSpVsnSspKUmSMv0CiitXrlivgfTnfMUzZszQzp07naa427hxo7X9Rry8vNSsWTOtWrVKBw8evOUNgl9++aU8PDy0fPlyp6ubs2bNylCb2WelaNGiKliwoFJTU53+spCZUqVKaceOHRmC/bXDP9LrpD+f/2tf7/R1t/v+L1u2rFauXKkHHnggS79016tXT/Xq1dNrr72muXPnqkuXLpo3b5569+59W+cHsoIxwEAuW7VqlV599VWFhoZa0wplJrNvEEv/AZ3+59T0YJFZIL0dn3zyidOfeL/44gsdOXJErVu3ttaVLVtWGzZs0OXLl611ixYtyjC9Vnba1qZNG6Wmplpzs6abNGmSHA6H0/n/ijZt2igxMdH6k7j0Z0idMmWKChQooMaNG+fIedKl/0n+doPk7erYsaPy58+v0aNHZ7iCbIzRiRMnJP05bVfRokU1bdo0p9czOjr6L7+nhg0bphMnTujDDz+01tWqVUsBAQGaMWNGhqEICxYs0OHDh7P9WoeFhSlfvnz67LPPnPp66NAhrVu3TjVr1rTWPfjgg3J1ddV7771nrTPGaNq0aSpWrNgNZylJN3LkSBlj1LVr10x/qYmLi7Om1MufP78cDodTMD9w4ECm3/jm7e2d4fnOnz+/HnroIX355ZeZzi187Ngx6/8jIyN1+PBhffPNN9a6ixcvOj330p+vd0BAgKZNm+b0/C9dulQ7d+60ZqbIrkcffVSpqal69dVXM2y7evWq1bdTp05leD9e/28acKdwBRi4i5YuXapdu3bp6tWrSkpK0qpVq7RixQqVKlVK33zzzU3/3DtmzBitXbtWUVFRKlWqlI4ePar33ntPxYsXt6ZqKlu2rPz8/DRt2jQVLFhQ3t7eCg8Pz9K0W5lJ/7Nxjx49lJSUpMmTJyssLMzpZprevXvriy++UKtWrfToo49q3759mj17ttMNXNltW7t27dS0aVO99NJLOnDggKpXr67vvvtOCxcu1KBBgzIc+3Y99dRT+uCDD9S9e3fFxcWpdOnS+uKLL7R+/XpNnjw5Wzc4Xe/w4cPWvM6XL1/Wr7/+qg8++ED+/v63HP6Q08qWLauxY8dq+PDhOnDggDp06KCCBQtq//79+vrrr/XUU0/pX//6l1xdXTV27Fj17dtXzZo102OPPab9+/dr1qxZ2RoDnJnWrVurSpUqmjhxovr37y9XV1e5ubnprbfeUrdu3VSnTh099thjKlKkiDZv3qyZM2eqWrVq1lRg19q0aZPGjh2bYX2TJk3UoEED9ezZUzNmzLDGyZ89e1bvvfeeUlJSNHz4cKu+ePHiGjRokN58801duXJFderU0YIFC7Ru3TrNmTPnlkN46tevr6lTp+qZZ55RhQoVnL4JbvXq1frmm2+sdkZFRWnixIlq1aqVHn/8cR09elRTp05VWFiYtmzZ4nTcWrVqaeXKlZo4caJCQkIUGhqq8PBwvfHGG/r+++8VHh6uPn36qFKlSjp58qR++eUXrVy50voluW/fvnr33XfVuXNnPffccwoODtacOXOsf1/Srwq7urrqP//5j3r06KHGjRurc+fO1jRopUuX1uDBg7PxCv+fxo0bq2/fvho3bpzi4+PVsmVLubq6as+ePZo/f77efvttPfzww/r444/13nvv6Z///KfKli2rs2fP6sMPP5SPj4/atGlzW+cGsuyuzzsB2FD6dE/pi5ubmwkKCjL/+Mc/zNtvv+003Va666fHiomJMQ8++KAJCQkxbm5uJiQkxHTu3Nn873//c9pv4cKFplKlSsbFxcVp2rHGjRubypUrZ9q+G02D9t///tcMHz7cBAQEGE9PTxMVFWV+//33DPtPmDDBFCtWzLi7u5sHHnjAbNq0KcMxb9a266dRMsaYs2fPmsGDB5uQkBDj6upqypUrZ958802TlpbmVKcbTIt1o+nZrpeUlGR69Ohh/P39jZubm6latWqmU7X9lWnQ8uXLZwICAkznzp2dpjYz5sbToGWlT+n7Xj81Xvr7bf/+/U7rv/zyS9OgQQPj7e1tvL29TYUKFUz//v3N7t27neree+89Exoaatzd3U3t2rXN2rVrM309b9T3Gz1P0dHRmU6Ft3TpUtO0aVPj4+NjXF1dTWhoqBkyZIg5depUhmNc+7xev7z66qvGGGOuXLlipkyZYmrUqGEKFChgChQoYJo2bWpWrVqV4Xipqanm9ddfN6VKlTJubm6mcuXKTtPiZUVcXJx5/PHHrfdqoUKFTPPmzc3HH39sUlNTrbqPPvrIlCtXzri7u5sKFSqYWbNmZfr679q1yzRq1Mh4enoaSU6veVJSkunfv78pUaKEcXV1NUFBQaZ58+Zm+vTpTsf47bffTFRUlPH09DRFixY1zz//vPnyyy+NJLNhwwan2s8++8zUrFnTuLu7m8KFC5suXbqYQ4cOOdV069bNeHt7Z9r/zD6/xvw5fV6tWrWMp6enKViwoKlatap54YUXzB9//GGMMeaXX34xnTt3NiVLljTu7u4mICDAtG3b1mzatOmWzznwVzmMyWN3iQAAgBw3efJkDR48WIcOHVKxYsVyuzlAriIAAwBwj0lJSXG6Ae3ixYuqWbOmUlNT9b///S8XWwbkDYwBBgDgHtOxY0eVLFlSNWrU0JkzZzR79mzt2rXrhlOTAXZDAAYA4B4TGRmpGTNmaM6cOUpNTVWlSpU0b948PfbYY7ndNCBPyNVp0MaNG6c6deqoYMGCCggIUIcOHbR7926nmiZNmsjhcDgt13/tZ0JCgqKiouTl5aWAgAANHTrUaa5HSVq9erXuv/9+ubu7KywsTNHR0Xe6ewAA5IpBgwZp27ZtOnfunFJSUhQXF0f4Ba6RqwF4zZo16t+/vzZs2KAVK1boypUratmypc6fP+9U16dPHx05csRaxo8fb21LTU1VVFSULl++rB9//FEff/yxoqOjNWLECKtm//79ioqKUtOmTRUfH69Bgwapd+/eWr58+V3rKwAAAPKGPHUT3LFjxxQQEKA1a9aoUaNGkv68AlyjRg1Nnjw5032WLl2qtm3b6o8//lBgYKAkadq0aRo2bJiOHTsmNzc3DRs2TIsXL3aaPLxTp046ffq0li1bdsf7BQAAgLwjT40BPnPmjKQ/J9+/1pw5czR79mwFBQWpXbt2euWVV+Tl5SVJio2NVdWqVa3wK/059unpp5/W9u3bVbNmTcXGxmb46sjIyEgNGjQo03ZcunTJ6Vto0tLSdPLkSRUpUiTHv2oWAAAAf50xRmfPnlVISIjT19tnJs8E4LS0NA0aNEgPPPCA03e/P/744ypVqpRCQkK0ZcsWDRs2TLt379ZXX30lSUpMTHQKv5Ksx4mJiTetSU5OzjBVjPTn2OTRo0fneB8BAABwZx08eFDFixe/aU2eCcD9+/fXtm3b9MMPPzitv/ZrMKtWrarg4GA1b95c+/bty7GvQ73e8OHDNWTIEOvxmTNnVLJkSR08eFA+Pj535JwAAAC4fcnJySpRokSWvsY+TwTgAQMGaNGiRVq7du0tE3t4eLgkae/evSpbtqyCgoL0008/OdUkJSVJkoKCgqz/pq+7tsbHxyfD1V9Jcnd3l7u7e4b1Pj4+BGAAAIA8LCvDVXN1FghjjAYMGKCvv/5aq1atUmho6C33iY+PlyQFBwdLkiIiIrR161YdPXrUqlmxYoV8fHxUqVIlqyYmJsbpOCtWrFBEREQO9QQAAAB/F7kagPv376/Zs2dr7ty5KliwoBITE5WYmKiUlBRJ0r59+/Tqq68qLi5OBw4c0DfffKMnn3xSjRo1UrVq1SRJLVu2VKVKldS1a1f9+uuvWr58uV5++WX179/fuorbr18//fbbb3rhhRe0a9cuvffee/r88881ePDgXOs7AAAAckeuToN2o0vUs2bNUvfu3XXw4EE98cQT2rZtm86fP68SJUron//8p15++WWnoQi///67nn76aa1evVre3t7q1q2b3njjDbm4/N8Ij9WrV2vw4MHasWOHihcvrldeeUXdu3fPUjuTk5Pl6+urM2fOMAQCAAAgD8pOXstT8wDnVQRgAACAvC07eS1Xh0AAAAAAdxsBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALaSJ74KGQBuR0JCgo4fP57bzYCN+fv7q2TJkrndDADZRAAG8LeUkJCgChUqKiXlQm43BTbm6emlXbt2EoKBvxkCMIC/pePHjysl5YLCe46UT3Dp3G4ObCj5yAFtnDlax48fJwADfzMEYAB/az7BpVW4ZPncbgYA4G+Em+AAAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICt5GoAHjdunOrUqaOCBQsqICBAHTp00O7du51qLl68qP79+6tIkSIqUKCAHnroISUlJTnVJCQkKCoqSl5eXgoICNDQoUN19epVp5rVq1fr/vvvl7u7u8LCwhQdHX2nuwcAAIA8KFcD8Jo1a9S/f39t2LBBK1as0JUrV9SyZUudP3/eqhk8eLC+/fZbzZ8/X2vWrNEff/yhjh07WttTU1MVFRWly5cv68cff9THH3+s6OhojRgxwqrZv3+/oqKi1LRpU8XHx2vQoEHq3bu3li9fflf7CwAAgNznkpsnX7ZsmdPj6OhoBQQEKC4uTo0aNdKZM2f00Ucfae7cuWrWrJkkadasWapYsaI2bNigevXq6bvvvtOOHTu0cuVKBQYGqkaNGnr11Vc1bNgwjRo1Sm5ubpo2bZpCQ0M1YcIESVLFihX1ww8/aNKkSYqMjLzr/QYAAEDuyVNjgM+cOSNJKly4sCQpLi5OV65cUYsWLayaChUqqGTJkoqNjZUkxcbGqmrVqgoMDLRqIiMjlZycrO3bt1s11x4jvSb9GNe7dOmSkpOTnRYAAADcG/JMAE5LS9OgQYP0wAMPqEqVKpKkxMREubm5yc/Pz6k2MDBQiYmJVs214Td9e/q2m9UkJycrJSUlQ1vGjRsnX19faylRokSO9BEAAAC5L88E4P79+2vbtm2aN29ebjdFw4cP15kzZ6zl4MGDud0kAAAA5JBcHQOcbsCAAVq0aJHWrl2r4sWLW+uDgoJ0+fJlnT592ukqcFJSkoKCgqyan376yel46bNEXFtz/cwRSUlJ8vHxkaenZ4b2uLu7y93dPUf6BgAAgLwlV68AG2M0YMAAff3111q1apVCQ0OdtteqVUuurq6KiYmx1u3evVsJCQmKiIiQJEVERGjr1q06evSoVbNixQr5+PioUqVKVs21x0ivST8GAAAA7CNXrwD3799fc+fO1cKFC1WwYEFrzK6vr688PT3l6+urXr16aciQISpcuLB8fHz07LPPKiIiQvXq1ZMktWzZUpUqVVLXrl01fvx4JSYm6uWXX1b//v2tq7j9+vXTu+++qxdeeEE9e/bUqlWr9Pnnn2vx4sW51ncAAADkjly9Avz+++/rzJkzatKkiYKDg63ls88+s2omTZqktm3b6qGHHlKjRo0UFBSkr776ytqeP39+LVq0SPnz51dERISeeOIJPfnkkxozZoxVExoaqsWLF2vFihWqXr26JkyYoBkzZjAFGgAAgA3l6hVgY8wtazw8PDR16lRNnTr1hjWlSpXSkiVLbnqcJk2aaPPmzdluIwAAAO4teWYWCAAAAOBuIAADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGwlV78IAzeXkJCg48eP53YzYGP+/v4qWbJkbjcDAIAcRQDOoxISElShQkWlpFzI7abAxjw9vbRr105CMADgnkIAzqOOHz+ulJQLCu85Uj7BpXO7ObCh5CMHtHHmaB0/fpwADAC4pxCA8zif4NIqXLJ8bjcDAADgnsFNcAAAALAVAjAAAABshQAMAAAAWyEAAwAAwFYIwAAAALAVAjAAAABshQAMAAAAWyEAAwAAwFYIwAAAALAVAjAAAABsha9CBgDgHpWQkKDjx4/ndjNgY/7+/ipZsmRuNyMDAjAAAPeghIQEVahQUSkpF3K7KbAxT08v7dq1M8+FYAIwAAD3oOPHjysl5YLCe46UT3Dp3G4ObCj5yAFtnDlax48fJwADAIC7xye4tAqXLJ/bzQDyFG6CAwAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK3kagBeu3at2rVrp5CQEDkcDi1YsMBpe/fu3eVwOJyWVq1aOdWcPHlSXbp0kY+Pj/z8/NSrVy+dO3fOqWbLli1q2LChPDw8VKJECY0fP/5Odw0AAAB5VK4G4PPnz6t69eqaOnXqDWtatWqlI0eOWMt///tfp+1dunTR9u3btWLFCi1atEhr167VU089ZW1PTk5Wy5YtVapUKcXFxenNN9/UqFGjNH369DvWLwAAAORdLrl58tatW6t169Y3rXF3d1dQUFCm23bu3Klly5bp559/Vu3atSVJU6ZMUZs2bfTWW28pJCREc+bM0eXLlzVz5ky5ubmpcuXKio+P18SJE52CMgAAAOwhz48BXr16tQICAlS+fHk9/fTTOnHihLUtNjZWfn5+VviVpBYtWihfvnzauHGjVdOoUSO5ublZNZGRkdq9e7dOnTqV6TkvXbqk5ORkpwUAAAD3hjwdgFu1aqVPPvlEMTEx+s9//qM1a9aodevWSk1NlSQlJiYqICDAaR8XFxcVLlxYiYmJVk1gYKBTTfrj9JrrjRs3Tr6+vtZSokSJnO4aAAAAckmuDoG4lU6dOln/X7VqVVWrVk1ly5bV6tWr1bx58zt23uHDh2vIkCHW4+TkZEIwAADAPSJPXwG+XpkyZeTv76+9e/dKkoKCgnT06FGnmqtXr+rkyZPWuOGgoCAlJSU51aQ/vtHYYnd3d/n4+DgtAAAAuDf8rQLwoUOHdOLECQUHB0uSIiIidPr0acXFxVk1q1atUlpamsLDw62atWvX6sqVK1bNihUrVL58eRUqVOjudgAAAAC5LlcD8Llz5xQfH6/4+HhJ0v79+xUfH6+EhASdO3dOQ4cO1YYNG3TgwAHFxMTowQcfVFhYmCIjIyVJFStWVKtWrdSnTx/99NNPWr9+vQYMGKBOnTopJCREkvT444/Lzc1NvXr10vbt2/XZZ5/p7bffdhriAAAAAPvI1QC8adMm1axZUzVr1pQkDRkyRDVr1tSIESOUP39+bdmyRe3bt9d9992nXr16qVatWlq3bp3c3d2tY8yZM0cVKlRQ8+bN1aZNGzVo0MBpjl9fX19999132r9/v2rVqqXnn39eI0aMYAo0AAAAm8rVm+CaNGkiY8wNty9fvvyWxyhcuLDmzp1705pq1app3bp12W4fAAAA7j1/qzHAAAAAwF9FAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICt3FYALlOmjE6cOJFh/enTp1WmTJm/3CgAAADgTrmtAHzgwAGlpqZmWH/p0iUdPnz4LzcKAAAAuFOy9UUY33zzjfX/y5cvl6+vr/U4NTVVMTExKl26dI41DgAAAMhp2QrAHTp0kCQ5HA5169bNaZurq6tKly6tCRMm5FjjAAAAgJyWrQCclpYmSQoNDdXPP/8sf3//O9IoAAAA4E7JVgBOt3///pxuBwAAAHBX3FYAlqSYmBjFxMTo6NGj1pXhdDNnzvzLDQMAAADuhNsKwKNHj9aYMWNUu3ZtBQcHy+Fw5HS7AAAAgDvitgLwtGnTFB0dra5du+Z0ewAAAIA76rbmAb58+bLq16+f020BAAAA7rjbCsC9e/fW3Llzc7otAAAAwB13W0MgLl68qOnTp2vlypWqVq2aXF1dnbZPnDgxRxoHAAAA5LTbCsBbtmxRjRo1JEnbtm1z2sYNcQAAAMjLbisAf//99zndDgAAAOCuuK0xwAAAAMDf1W1dAW7atOlNhzqsWrXqthsEAAAA3Em3FYDTx/+mu3LliuLj47Vt2zZ169YtJ9oFAAAA3BG3FYAnTZqU6fpRo0bp3Llzf6lBAAAAwJ2Uo2OAn3jiCc2cOTMnDwkAAADkqBwNwLGxsfLw8MjJQwIAAAA56raGQHTs2NHpsTFGR44c0aZNm/TKK6/kSMMAAACAO+G2ArCvr6/T43z58ql8+fIaM2aMWrZsmSMNAwAAAO6E2wrAs2bNyul2AAAAAHfFbQXgdHFxcdq5c6ckqXLlyqpZs2aONAoAAAC4U24rAB89elSdOnXS6tWr5efnJ0k6ffq0mjZtqnnz5qlo0aI52UYAAAAgx9zWLBDPPvuszp49q+3bt+vkyZM6efKktm3bpuTkZA0cODCn2wgAAADkmNu6Arxs2TKtXLlSFStWtNZVqlRJU6dO5SY4AAAA5Gm3dQU4LS1Nrq6uGda7uroqLS3tLzcKAAAAuFNuKwA3a9ZMzz33nP744w9r3eHDhzV48GA1b948xxoHAAAA5LTbCsDvvvuukpOTVbp0aZUtW1Zly5ZVaGiokpOTNWXKlJxuIwAAAJBjbmsMcIkSJfTLL79o5cqV2rVrlySpYsWKatGiRY42DgAAAMhp2boCvGrVKlWqVEnJyclyOBz6xz/+oWeffVbPPvus6tSpo8qVK2vdunV3qq0AAADAX5atADx58mT16dNHPj4+Gbb5+vqqb9++mjhxYo41DgAAAMhp2QrAv/76q1q1anXD7S1btlRcXNxfbhQAAABwp2QrACclJWU6/Vk6FxcXHTt27C83CgAAALhTshWAixUrpm3btt1w+5YtWxQcHPyXGwUAAADcKdkKwG3atNErr7yiixcvZtiWkpKikSNHqm3btjnWOAAAACCnZWsatJdffllfffWV7rvvPg0YMEDly5eXJO3atUtTp05VamqqXnrppTvSUAAAACAnZCsABwYG6scff9TTTz+t4cOHyxgjSXI4HIqMjNTUqVMVGBh4RxoKAAAA5IRsfxFGqVKltGTJEp06dUp79+6VMUblypVToUKF7kT7AAAAgBx1W98EJ0mFChVSnTp1crItAAAAwB2XrZvgAAAAgL87AjAAAABshQAMAAAAWyEAAwAAwFZyNQCvXbtW7dq1U0hIiBwOhxYsWOC03RijESNGKDg4WJ6enmrRooX27NnjVHPy5El16dJFPj4+8vPzU69evXTu3Dmnmi1btqhhw4by8PBQiRIlNH78+DvdNQAAAORRuRqAz58/r+rVq2vq1KmZbh8/frzeeecdTZs2TRs3bpS3t7ciIyOdvomuS5cu2r59u1asWKFFixZp7dq1euqpp6ztycnJatmypUqVKqW4uDi9+eabGjVqlKZPn37H+wcAAIC857anQcsJrVu3VuvWrTPdZozR5MmT9fLLL+vBBx+UJH3yyScKDAzUggUL1KlTJ+3cuVPLli3Tzz//rNq1a0uSpkyZojZt2uitt95SSEiI5syZo8uXL2vmzJlyc3NT5cqVFR8fr4kTJzoFZQAAANhDnh0DvH//fiUmJqpFixbWOl9fX4WHhys2NlaSFBsbKz8/Pyv8SlKLFi2UL18+bdy40app1KiR3NzcrJrIyEjt3r1bp06dyvTcly5dUnJystMCAACAe0OeDcCJiYmSlOGrlQMDA61tiYmJCggIcNru4uKiwoULO9Vkdoxrz3G9cePGydfX11pKlCjx1zsEAACAPCHPBuDcNHz4cJ05c8ZaDh48mNtNAgAAQA7JswE4KChIkpSUlOS0PikpydoWFBSko0ePOm2/evWqTp486VST2TGuPcf13N3d5ePj47QAAADg3pBnA3BoaKiCgoIUExNjrUtOTtbGjRsVEREhSYqIiNDp06cVFxdn1axatUppaWkKDw+3atauXasrV65YNStWrFD58uVVqFChu9QbAAAA5BW5GoDPnTun+Ph4xcfHS/rzxrf4+HglJCTI4XBo0KBBGjt2rL755htt3bpVTz75pEJCQtShQwdJUsWKFdWqVSv16dNHP/30k9avX68BAwaoU6dOCgkJkSQ9/vjjcnNzU69evbR9+3Z99tlnevvttzVkyJBc6jUAAAByU65Og7Zp0yY1bdrUepweSrt166bo6Gi98MILOn/+vJ566imdPn1aDRo00LJly+Th4WHtM2fOHA0YMEDNmzdXvnz59NBDD+mdd96xtvv6+uq7775T//79VatWLfn7+2vEiBFMgQYAAGBTuRqAmzRpImPMDbc7HA6NGTNGY8aMuWFN4cKFNXfu3Juep1q1alq3bt1ttxMAAAD3jjw7BhgAAAC4EwjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsBUCMAAAAGyFAAwAAABbIQADAADAVgjAAAAAsJU8HYBHjRolh8PhtFSoUMHafvHiRfXv319FihRRgQIF9NBDDykpKcnpGAkJCYqKipKXl5cCAgI0dOhQXb169W53BQAAAHmES2434FYqV66slStXWo9dXP6vyYMHD9bixYs1f/58+fr6asCAAerYsaPWr18vSUpNTVVUVJSCgoL0448/6siRI3ryySfl6uqq119//a73BQAAALkvzwdgFxcXBQUFZVh/5swZffTRR5o7d66aNWsmSZo1a5YqVqyoDRs2qF69evruu++0Y8cOrVy5UoGBgapRo4ZeffVVDRs2TKNGjZKbm9vd7g4AAAByWZ4eAiFJe/bsUUhIiMqUKaMuXbooISFBkhQXF6crV66oRYsWVm2FChVUsmRJxcbGSpJiY2NVtWpVBQYGWjWRkZFKTk7W9u3bb3jOS5cuKTk52WkBAADAvSFPB+Dw8HBFR0dr2bJlev/997V//341bNhQZ8+eVWJiotzc3OTn5+e0T2BgoBITEyVJiYmJTuE3fXv6thsZN26cfH19raVEiRI52zEAAADkmjw9BKJ169bW/1erVk3h4eEqVaqUPv/8c3l6et6x8w4fPlxDhgyxHicnJxOCAQAA7hF5+grw9fz8/HTfffdp7969CgoK0uXLl3X69GmnmqSkJGvMcFBQUIZZIdIfZzauOJ27u7t8fHycFgAAANwb/lYB+Ny5c9q3b5+Cg4NVq1Ytubq6KiYmxtq+e/duJSQkKCIiQpIUERGhrVu36ujRo1bNihUr5OPjo0qVKt319gMAACD35ekhEP/617/Url07lSpVSn/88YdGjhyp/Pnzq3PnzvL19VWvXr00ZMgQFS5cWD4+Pnr22WcVERGhevXqSZJatmypSpUqqWvXrho/frwSExP18ssvq3///nJ3d8/l3gEAACA35OkAfOjQIXXu3FknTpxQ0aJF1aBBA23YsEFFixaVJE2aNEn58uXTQw89pEuXLikyMlLvvfeetX/+/Pm1aNEiPf3004qIiJC3t7e6deumMWPG5FaXAAAAkMvydACeN2/eTbd7eHho6tSpmjp16g1rSpUqpSVLluR00wAAAPA39bcaAwwAAAD8VQRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZCAAYAAICtEIABAABgKwRgAAAA2AoBGAAAALZiqwA8depUlS5dWh4eHgoPD9dPP/2U200CAADAXWabAPzZZ59pyJAhGjlypH755RdVr15dkZGROnr0aG43DQAAAHeRbQLwxIkT1adPH/Xo0UOVKlXStGnT5OXlpZkzZ+Z20wAAAHAXueR2A+6Gy5cvKy4uTsOHD7fW5cuXTy1atFBsbGyG+kuXLunSpUvW4zNnzkiSkpOT73xj/79z585Jkk7+vltXL6XctfMC6ZITEyT9+V68m+/9rOIzgtzGZwS4ubv9GUk/hzHmlrUOk5Wqv7k//vhDxYoV048//qiIiAhr/QsvvKA1a9Zo48aNTvWjRo3S6NGj73YzAQAA8BcdPHhQxYsXv2mNLa4AZ9fw4cM1ZMgQ63FaWppOnjypIkWKyOFw5GLLkFXJyckqUaKEDh48KB8fn9xuDpDn8BkBbo3Pyd+LMUZnz55VSEjILWttEYD9/f2VP39+JSUlOa1PSkpSUFBQhnp3d3e5u7s7rfPz87uTTcQd4uPjwz9awE3wGQFujc/J34evr2+W6mxxE5ybm5tq1aqlmJgYa11aWppiYmKchkQAAADg3meLK8CSNGTIEHXr1k21a9dW3bp1NXnyZJ0/f149evTI7aYBAADgLrJNAH7sscd07NgxjRgxQomJiapRo4aWLVumwMDA3G4a7gB3d3eNHDkyw1AWAH/iMwLcGp+Te5ctZoEAAAAA0tliDDAAAACQjgAMAAAAWyEAAwAAwFYIwAAAALAVAjDypKlTp6p06dLy8PBQeHi4fvrppxvWHjhwQL169VJoaKg8PT1VtmxZjRw5UpcvX3aq27Jlixo2bCgPDw+VKFFC48ePz3Cs+fPnq0KFCvLw8FDVqlW1ZMmSHO8bcDPjxo1TnTp1VLBgQQUEBKhDhw7avXu3U83FixfVv39/FSlSRAUKFNBDDz2U4Yt+EhISFBUVJS8vLwUEBGjo0KG6evVqltsxb948ORwOdejQwWm9MUYjRoxQcHCwPD091aJFC+3Zs8ep5uTJk+rSpYt8fHzk5+enXr166dy5c9l7IoBccOnSJdWoUUMOh0Px8fFO2/gZco8xQB4zb9484+bmZmbOnGm2b99u+vTpY/z8/ExSUlKm9UuXLjXdu3c3y5cvN/v27TMLFy40AQEB5vnnn7dqzpw5YwIDA02XLl3Mtm3bzH//+1/j6elpPvjgA6tm/fr1Jn/+/Gb8+PFmx44d5uWXXzaurq5m69atd7zPQLrIyEgza9Yss23bNhMfH2/atGljSpYsac6dO2fV9OvXz5QoUcLExMSYTZs2mXr16pn69etb269evWqqVKliWrRoYTZv3myWLFli/P39zfDhw7PUhv3795tixYqZhg0bmgcffNBp2xtvvGF8fX3NggULzK+//mrat29vQkNDTUpKilXTqlUrU716dbNhwwazbt06ExYWZjp37vzXnhggmw4fPmyuXLmSrX0GDhxoWrdubSSZzZs3W+v5GXLvIQAjz6lbt67p37+/9Tg1NdWEhISYcePGZfkY48ePN6Ghodbj9957zxQqVMhcunTJWjds2DBTvnx56/Gjjz5qoqKinI4THh5u+vbtezvdAHLE0aNHjSSzZs0aY4wxp0+fNq6urmb+/PlWzc6dO40kExsba4wxZsmSJSZfvnwmMTHRqnn//feNj4+P02cgM1evXjX169c3M2bMMN26dXMKwGlpaSYoKMi8+eab1rrTp08bd3d389///tcYY8yOHTuMJPPzzz9bNUuXLjUOh8McPnz49p8IIJtGjRplAgMDzfPPP2+2bNlyy/olS5aYChUqmO3bt2cIwPwMufcwBAJ5yuXLlxUXF6cWLVpY6/Lly6cWLVooNjY2y8c5c+aMChcubD2OjY1Vo0aN5ObmZq2LjIzU7t27derUKavm2vOm12TnvEBOO3PmjCRZ7+e4uDhduXLF6b1aoUIFlSxZ0nqvxsbGqmrVqk5f9BMZGank5GRt3779pucbM2aMAgIC1KtXrwzb9u/fr8TERKdz+/r6Kjw83Oncfn5+ql27tlXTokUL5cuXTxs3bsxu94HbNmzYML399tvauXOn7r//ft1///165513dOzYsQy1SUlJ6tOnjz799FN5eXll2M7PkHsPARh5yvHjx5WamprhG/oCAwOVmJiYpWPs3btXU6ZMUd++fa11iYmJmR4zfdvNarJ6XiCnpaWladCgQXrggQdUpUoVSX++T93c3OTn5+dUe+17NSvv98z88MMP+uijj/Thhx9muj1935t9ThITExUQEOC03cXFRYULF+azhLvKw8NDjz32mBYvXqzDhw/rySefVHR0tIoVK6YOHTro66+/1tWrV2WMUffu3dWvXz+nX9yuxc+Qew8BGH8r/fr1U4ECBazleocPH1arVq30yCOPqE+fPrnQQiDn9O/fX9u2bdO8efNy9LgJCQlOn6PXX39dZ8+eVdeuXfXhhx/K398/R88H5LaAgAANGjRIv/zyixYuXKjY2Fh17NhR27Zt05QpU3T27FkNHz48t5uJu8gltxsAXMvf31/58+fPcEd7UlKSgoKCNGbMGP3rX//KdN8//vhDTZs2Vf369TV9+nSnbUFBQZkeM33bzWrStwN304ABA7Ro0SKtXbtWxYsXt9YHBQXp8uXLOn36tNNV4Gvfq0FBQRlmTrn2/R4SEuJ0h3vhwoW1b98+HThwQO3atbPWp6WlSfrzCu7u3but4yclJSk4ONjp2DVq1LCOf/ToUadzX716VSdPnuSzhFxz9uxZffHFF/r000+1du1aNW7cWN26dVOlSpU0atQoxcbGyt3d3Wmf2rVrq0uXLvr444/5GXIP4gow8hQ3NzfVqlVLMTEx1rq0tDTFxMQoIiJCAQEBCgsLs5Z0hw8fVpMmTVSrVi3NmjVL+fI5v7UjIiK0du1aXblyxVq3YsUKlS9fXoUKFbJqrj1vek1ERMSd6CqQKWOMBgwYoK+//lqrVq1SaGio0/ZatWrJ1dXV6b26e/duJSQkWO/ViIgIbd261SmIrlixQj4+PqpUqZJcXFycPkeFCxdWhQoVtHXrVsXHx1tL+/bt1bRpU8XHx6tEiRIKDQ1VUFCQ07mTk5O1ceNGp3OfPn1acXFxVs2qVauUlpam8PDwO/KcAZlJTU3V0qVL9fjjjyswMFBvvPGGmjdvrt9++00xMTF68skn5ebmpnfeeUe//vqr9b5Pn7rss88+02uvvSaJnyH3pNy+Cw+43rx584y7u7uJjo42O3bsME899ZTx8/NzuqP9WocOHTJhYWGmefPm5tChQ+bIkSPWku706dMmMDDQdO3a1Wzbts3MmzfPeHl5ZZjCxsXFxbz11ltm586dZuTIkUxhg7vu6aefNr6+vmb16tVO7+ULFy5YNf369TMlS5Y0q1atMps2bTIREREmIiLC2p4+DVrLli1NfHy8WbZsmSlatGiWp0FLd/0sEMb8OQ2an5+fWbhwodmyZYt58MEHM50GrWbNmmbjxo3mhx9+MOXKlWMaNNx1Y8aMMb6+vuapp54y69evz/J++/fvzzALBD9D7j0EYORJU6ZMMSVLljRubm6mbt26ZsOGDTesnTVrlpGU6XKtX3/91TRo0MC4u7ubYsWKmTfeeCPDsT7//HNz3333GTc3N1O5cmWzePHiHO8bcDM3ei/PmjXLqklJSTHPPPOMKVSokPHy8jL//Oc/nX7hM8aYAwcOmNatWxtPT0/j7+9vnn/++WzPiZpZAE5LSzOvvPKKCQwMNO7u7qZ58+Zm9+7dTjUnTpwwnTt3NgUKFDA+Pj6mR48e5uzZs9k6N/BX7d+/3+kXs+zsd30ANoafIfcahzHG5MaVZwAAACA3MAYYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYAAAAtkIABgAAgK0QgAEAAGArBGAAAADYCgEYgC0cOHBADodD8fHxud0UrV69Wg6HQ6dPn87tptwVjRo10ty5c+/qOaOjo+Xn55djx6tXr56+/PLLHDsegNxFAAbwt9e9e3c5HA5rKVKkiFq1aqUtW7ZYNSVKlNCRI0dUpUqVXGxp1pUuXdrqj5eXl6pWraoZM2Y41dws5DkcDi1YsEBS1sL/nDlzVL16dXl5eSk4OFg9e/bUiRMnnGrmz5+vChUqyMPDQ1WrVtWSJUtu2Y9vvvlGSUlJ6tSpk9P6zZs365FHHlFgYKA8PDxUrlw59enTR//73/9ueczc8PLLL+vFF19UWlpabjcFQA4gAAO4J7Rq1UpHjhzRkSNHFBMTIxcXF7Vt29banj9/fgUFBcnFxSUXW5k9Y8aM0ZEjR7Rt2zY98cQT6tOnj5YuXZrj51m/fr2efPJJ9erVS9u3b9f8+fP1008/qU+fPlbNjz/+qM6dO6tXr17avHmzOnTooA4dOmjbtm03PfY777yjHj16KF++//txs2jRItWrV0+XLl3SnDlztHPnTs2ePVu+vr565ZVXcrx/OaF169Y6e/bsHXn+Adx9BGAA9wR3d3cFBQUpKChINWrU0IsvvqiDBw/q2LFjkjJeBU0fhhATE6PatWvLy8tL9evX1+7du61jjho1SjVq1NCnn36q0qVLy9fXV506ddLZs2etmrS0NI0bN06hoaHy9PRU9erV9cUXXzi1bcmSJbrvvvvk6emppk2b6sCBA1nqU8GCBRUUFKQyZcpo2LBhKly4sFasWPHXnqhMxMbGqnTp0ho4cKBCQ0PVoEED9e3bVz/99JNV8/bbb6tVq1YaOnSoKlasqFdffVX333+/3n333Rse99ixY1q1apXatWtnrbtw4YJ69OihNm3a6JtvvlGLFi0UGhqq8PBwvfXWW/rggw9kjFFYWJjeeustp+PFx8fL4XBo7969kqTTp0+rb9++1lXkKlWqaNGiRTdsz8KFC3X//ffLw8NDZcqU0ejRo3X16lVJkjFGo0aNUsmSJeXu7q6QkBANHDjQ2jd//vxq06aN5s2bl70nF0CeRAAGcM85d+6cZs+erbCwMBUpUuSmtS+99JImTJigTZs2ycXFRT179nTavm/fPi1YsECLFi3SokWLtGbNGr3xxhvW9nHjxumTTz7RtGnTtH37dg0ePFhPPPGE1qxZI0k6ePCgOnbsqHbt2ik+Pl69e/fWiy++mK3+pKWl6csvv9SpU6fk5uaWrX2zIiIiQgcPHtSSJUtkjFFSUpK++OILtWnTxqqJjY1VixYtnPaLjIxUbGzsDY/7ww8/yMvLSxUrVrTWLV++XMePH9cLL7yQ6T5+fn5yOBzq2bOnZs2a5bRt1qxZatSokcLCwpSWlqbWrVtr/fr1mj17tnbs2KE33nhD+fPnz/S469at05NPPqnnnntOO3bs0AcffKDo6Gi99tprkqQvv/xSkyZN0gcffKA9e/ZowYIFqlq1qtMx6tatq3Xr1t2wvwD+RgwA/M1169bN5M+f33h7extvb28jyQQHB5u4uDirZv/+/UaS2bx5szHGmO+//95IMitXrrRqFi9ebCSZlJQUY4wxI0eONF5eXiY5OdmqGTp0qAkPDzfGGHPx4kXj5eVlfvzxR6f29OrVy3Tu3NkYY8zw4cNNpUqVnLYPGzbMSDKnTp26YZ9KlSpl3NzcjLe3t3FxcTGSTOHChc2ePXusmlmzZhlfX99M95dkvv7660z7npnPP//cFChQwDpXu3btzOXLl63trq6uZu7cuU77TJ061QQEBNzwmJMmTTJlypRxWvef//zHSDInT5684X7GGHP48GGTP39+s3HjRmOMMZcvXzb+/v4mOjraGGPM8uXLTb58+czu3bsz3f/656Z58+bm9ddfd6r59NNPTXBwsDHGmAkTJpj77rvPqc/XW7hwocmXL59JTU29adsB5H1cAQZwT2jatKni4+MVHx+vn376SZGRkWrdurV+//33m+5XrVo16/+Dg4MlSUePHrXWlS5dWgULFnSqSd++d+9eXbhwQf/4xz9UoEABa/nkk0+0b98+SdLOnTsVHh7udM6IiIgs9Wno0KGKj4/XqlWrFB4erkmTJiksLCxL+2bHjh079Nxzz2nEiBGKi4vTsmXLdODAAfXr1+8vHTclJUUeHh5O64wxWdo3JCREUVFRmjlzpiTp22+/1aVLl/TII49I+nM4RPHixXXfffdl6Xi//vqrxowZ4/Q69enTR0eOHNGFCxf0yCOPKCUlRWXKlFGfPn309ddfW8Mj0nl6eiotLU2XLl3K0jkB5F1/n7tBAOAmvL29ncLhjBkz5Ovrqw8//FBjx4694X6urq7W/zscDklyutP/2u3pNenbz507J0lavHixihUr5lTn7u5+mz35P/7+/goLC1NYWJjmz5+vqlWrqnbt2qpUqZIkycfHR+fPn1daWprTTWbp06v5+vpm6Tzjxo3TAw88oKFDh0r685cCb29vNWzYUGPHjlVwcLCCgoKUlJTktF9SUpKCgoJu2v5Tp045rUsPrLt27brlLwK9e/dW165dNWnSJM2aNUuPPfaYvLy8JP0ZRrPj3LlzGj16tDp27Jhhm4eHh0qUKKHdu3dr5cqVWrFihZ555hm9+eabWrNmjfUeOHnypLy9vbN9bgB5D1eAAdyTHA6H8uXLp5SUlDt2jkqVKsnd3V0JCQlWUE1fSpQoIUmqWLGi081kkrRhw4Zsn6tEiRJ67LHHNHz4cGtd+fLldfXq1QzTm/3yyy+SlOWroxcuXHAK0JKssbTpV2wjIiIUExPjVLNixYqbhtiaNWsqMTHRKQS3bNlS/v7+Gj9+fKb7XDs3cps2beTt7a33339fy5YtcxqfXa1aNR06dCjL06bdf//92r17d4bXKSwszOq7p6en2rVrp3feeUerV69WbGystm7dah1j27ZtqlmzZpbOByBv4wowgHvCpUuXlJiYKEk6deqU3n33XZ07d85pBoKcVrBgQf3rX//S4MGDlZaWpgYNGujMmTNav369fHx81K1bN/Xr108TJkzQ0KFD1bt3b8XFxSk6Ovq2zvfcc8+pSpUq2rRpk2rXrq3KlSurZcuW6tmzpyZMmKAyZcpo9+7dGjRokB577LEMV6WvneEiXeXKldWuXTv16dNH77//viIjI3XkyBENGjRIdevWVUhIiHXuxo0ba8KECYqKitK8efO0adMmTZ8+/YbtrVmzpvz9/bV+/XprSjpvb2/NmDFDjzzyiNq3b6+BAwcqLCxMx48f1+eff66EhARrpoX8+fOre/fuGj58uMqVK+cUths3bqxGjRrpoYce0sSJExUWFqZdu3bJ4XCoVatWGdoyYsQItW3bViVLltTDDz+sfPny6ddff9W2bds0duxYRUdHKzU1VeHh4fLy8tLs2bPl6empUqVKWcdYt26dWrZsmY1XDECelduDkAHgr+rWrZuRZC0FCxY0derUMV988YVVc6Ob4K69EW3z5s1Gktm/f78x5s+b4KpXr+50rkmTJplSpUpZj9PS0szkyZNN+fLljaurqylatKiJjIw0a9assWq+/fZbExYWZtzd3U3Dhg3NzJkzs3QT3KRJkzKsj4yMNK1bt7Yenzp1ygwcONCULVvWeHp6mnLlypkXXnjBnD17NkPfM1sOHjxojDHmnXfeMZUqVTKenp4mODjYdOnSxRw6dMjp3J9//rm57777jJubm6lcubJZvHjxDduf7oUXXjCdOnXKsP7nn382HTt2NEWLFjXu7u4mLCzMPPXUU043+RljzL59+4wkM378+AzHOHHihOnRo4cpUqSI8fDwMFWqVDGLFi0yxmR+g+CyZctM/fr1jaenp/Hx8TF169Y106dPN8YY8/XXX5vw8HDj4+NjvL29Tb169ZxukDx06JBxdXW1ni8Af28OY7J4RwIAANmUmJioypUr65dffnG6mppV69atU/PmzXXw4EEFBgbegRZmzbBhw3Tq1KmbXvEG8PfBGGAAwB0TFBSkjz76SAkJCdna79KlSzp06JBGjRplfWVybgoICNCrr76aq20AkHO4AgwAyHOio6PVq1cv1ahRQ998802G8cwA8FcQgAEAAGArDIEAAACArRCAAQAAYCsEYAAAANgKARgAAAC2QgAGAACArRCAAQAAYCsEYAAAANgKARgAAAC28v8Arq/DHQEPAeQAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"  # Adjust if needed\n",
    "data_loader = mpy.loadmat(file_path)\n",
    "df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "# Function to trim history arrays to the last 120 cycles\n",
    "def trim_history(row, min_length=120):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "\n",
    "    if len(history_cap) < min_length or len(history_cycles) < min_length:\n",
    "        return pd.Series({\"History\": np.nan, \"History_Cycle\": np.nan})\n",
    "\n",
    "    return pd.Series({\n",
    "        \"History\": history_cap[-min_length:],  # Keep last 120 values\n",
    "        \"History_Cycle\": history_cycles[-min_length:]  # Keep last 120 cycles\n",
    "    })\n",
    "\n",
    "# Apply trimming\n",
    "df[[\"History\", \"History_Cycle\"]] = df.apply(trim_history, axis=1)\n",
    "\n",
    "# Drop NaN values (for sequences shorter than 120 cycles)\n",
    "df = df.dropna(subset=[\"History\"])\n",
    "\n",
    "# Compute EOL80 and RUL80\n",
    "def compute_eol_and_rul80(row):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "    target_cap = np.array(row[\"Target_expanded\"])\n",
    "    target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "    eol80_cycle, rul80 = np.nan, np.nan\n",
    "    if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "    \n",
    "    initial_capacity = history_cap[0]\n",
    "    threshold = 0.8 * initial_capacity\n",
    "\n",
    "    if history_cap[-1] <= threshold:\n",
    "        return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "    \n",
    "    if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "    if len(below_threshold_indices) > 0:\n",
    "        eol80_index = below_threshold_indices[0]\n",
    "        eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "    if not pd.isna(eol80_cycle):\n",
    "        last_history_cycle = history_cycles[-1]\n",
    "        rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "    return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "# Drop NaN values for RUL80 before binning\n",
    "df_filtered = df.dropna(subset=[\"RUL80\"])\n",
    "\n",
    "\n",
    "# Define bins and labesl, but with categories starting from 0-200 and increasing by 200\n",
    "bins = [0, 200, 400, np.inf]\n",
    "labels = [\"0-200\", \"200-400\", \">400\"]\n",
    "\n",
    "\n",
    "# Apply binning\n",
    "df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Plot histogram of binned RUL80 values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_filtered[\"RUL80_binned\"], discrete=True, shrink=0.8)\n",
    "plt.xlabel(\"Binned RUL80 (Cycles)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Binned RUL80 Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot histogram of continuous RUL80 values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_filtered[\"RUL80\"], bins=30, kde=True)\n",
    "plt.xlabel(\"RUL80 (Cycles)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of RUL80 Values\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/3305826383.py:1: DeprecationWarning: `import kerastuner` is deprecated, please use `import keras_tuner`.\n",
      "  import kerastuner as kt\n",
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/3305826383.py:23: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(None)\n",
    "random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "labels = [\"0-200\", \"200-400\", \">400\"]\n",
    "\n",
    "# Set bins and labels\n",
    "label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare sequences\n",
    "X = np.array(df_filtered[\"History\"].tolist())  # Direct NumPy conversion (avoiding unnecessary padding)\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize sequences\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = np.array(df_train[\"History\"].tolist()), np.array(df_test[\"History\"].tolist())\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize training/testing separately\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/2731678857.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
      "/Users/johannesherstad/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/numerical_utils.py:87: RuntimeWarning: invalid value encountered in cast\n",
      "  x = np.array(x, dtype=\"int64\")\n",
      "/Users/johannesherstad/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/numerical_utils.py:87: RuntimeWarning: invalid value encountered in cast\n",
      "  x = np.array(x, dtype=\"int64\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 10\n",
      "conv1_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 16, 'max_value': 64, 'step': 16, 'sampling': 'linear'}\n",
      "conv1_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5, 7], 'ordered': True}\n",
      "l2_reg (Choice)\n",
      "{'default': 0.001, 'conditions': [], 'values': [0.001, 0.0005, 0.0001], 'ordered': True}\n",
      "conv2_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 32, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "conv2_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "conv3_filters (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 128, 'step': 32, 'sampling': 'linear'}\n",
      "conv3_kernel_size (Choice)\n",
      "{'default': 3, 'conditions': [], 'values': [3, 5], 'ordered': True}\n",
      "dense_units (Int)\n",
      "{'default': None, 'conditions': [], 'min_value': 64, 'max_value': 256, 'step': 64, 'sampling': 'linear'}\n",
      "dropout_rate (Float)\n",
      "{'default': 0.3, 'conditions': [], 'min_value': 0.3, 'max_value': 0.7, 'step': 0.1, 'sampling': 'linear'}\n",
      "learning_rate (Choice)\n",
      "{'default': 0.01, 'conditions': [], 'values': [0.01, 0.001, 0.0001], 'ordered': True}\n",
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "48                |48                |conv1_filters\n",
      "5                 |5                 |conv1_kernel_size\n",
      "0.0001            |0.0001            |l2_reg\n",
      "64                |64                |conv2_filters\n",
      "3                 |3                 |conv2_kernel_size\n",
      "128               |128               |conv3_filters\n",
      "3                 |3                 |conv3_kernel_size\n",
      "192               |192               |dense_units\n",
      "0.3               |0.3               |dropout_rate\n",
      "0.0001            |0.0001            |learning_rate\n",
      "\n",
      "Epoch 1/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9926 - loss: 0.0673 - val_accuracy: 1.0000 - val_loss: 0.0446\n",
      "Epoch 2/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0427 - val_accuracy: 1.0000 - val_loss: 0.0396\n",
      "Epoch 3/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0387 - val_accuracy: 1.0000 - val_loss: 0.0357\n",
      "Epoch 4/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0348 - val_accuracy: 1.0000 - val_loss: 0.0318\n",
      "Epoch 5/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0308 - val_accuracy: 1.0000 - val_loss: 0.0277\n",
      "Epoch 6/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0267 - val_accuracy: 1.0000 - val_loss: 0.0237\n",
      "Epoch 7/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0228 - val_accuracy: 1.0000 - val_loss: 0.0199\n",
      "Epoch 8/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0189 - val_accuracy: 1.0000 - val_loss: 0.0162\n",
      "Epoch 9/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0154 - val_accuracy: 1.0000 - val_loss: 0.0129\n",
      "Epoch 10/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0122 - val_accuracy: 1.0000 - val_loss: 0.0100\n",
      "Epoch 11/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0093 - val_accuracy: 1.0000 - val_loss: 0.0075\n",
      "Epoch 12/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0069 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
      "Epoch 13/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0050 - val_accuracy: 1.0000 - val_loss: 0.0038\n",
      "Epoch 14/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0035 - val_accuracy: 1.0000 - val_loss: 0.0026\n",
      "Epoch 15/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0024 - val_accuracy: 1.0000 - val_loss: 0.0018\n",
      "Epoch 16/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0016 - val_accuracy: 1.0000 - val_loss: 0.0012\n",
      "Epoch 17/100\n",
      "\u001b[1m582/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 0.0011 - val_accuracy: 1.0000 - val_loss: 7.8185e-04\n",
      "Epoch 18/100\n",
      "\u001b[1m578/582\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 1.0000 - loss: 7.0272e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 128\u001b[0m\n\u001b[1;32m    125\u001b[0m tuner\u001b[38;5;241m.\u001b[39msearch_space_summary()\n\u001b[1;32m    127\u001b[0m \u001b[38;5;66;03m# Run the tuner search with increased epochs and early stopping patience\u001b[39;00m\n\u001b[0;32m--> 128\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Increased from 50 to 100 epochs\u001b[39;49;00m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_accuracy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    134\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# Display the best hyperparameters after tuning\u001b[39;00m\n\u001b[1;32m    137\u001b[0m tuner\u001b[38;5;241m.\u001b[39mresults_summary()\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[0;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[0;32m--> 234\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 274\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_and_update_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[1;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[0;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[1;32m    242\u001b[0m     ):\n\u001b[1;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[1;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[1;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    255\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[1;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[0;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_and_fit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcopied_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[0;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[1;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[1;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[0;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhypermodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras_tuner/src/engine/hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[0;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[1;32m    127\u001b[0m \n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:395\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[1;32m    386\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[1;32m    387\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    393\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    394\u001b[0m     )\n\u001b[0;32m--> 395\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    398\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    399\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    406\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    407\u001b[0m }\n\u001b[1;32m    408\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:484\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[1;32m    483\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(step)\n\u001b[0;32m--> 484\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    485\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(step, logs)\n\u001b[1;32m    486\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/backend/tensorflow/trainer.py:219\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunction\u001b[39m(iterator):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m    217\u001b[0m         iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[1;32m    218\u001b[0m     ):\n\u001b[0;32m--> 219\u001b[0m         opt_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mhas_value():\n\u001b[1;32m    221\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[1;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[0;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[1;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[1;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[1;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m     args,\n\u001b[1;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1326\u001b[0m     executing_eagerly)\n\u001b[1;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[1;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[1;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[1;32m    261\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[1;32m   1515\u001b[0m   )\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(None)\n",
    "random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "labels = [\"0-200\", \"200-400\", \">400\"]\n",
    "\n",
    "# Set bins and labels\n",
    "label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare sequences\n",
    "X = np.array(df_filtered[\"History\"].tolist())  # Direct NumPy conversion (avoiding unnecessary padding)\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize sequences\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = np.array(df_train[\"History\"].tolist()), np.array(df_test[\"History\"].tolist())\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize training/testing separately\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_mapping))\n",
    "\n",
    "# Define the model-building function for Keras Tuner with a third Conv layer\n",
    "def build_model(hp):\n",
    "    input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "    \n",
    "    # First Conv Layer\n",
    "    x = Conv1D(\n",
    "        filters=hp.Int('conv1_filters', min_value=16, max_value=64, step=16),\n",
    "        kernel_size=hp.Choice('conv1_kernel_size', values=[3, 5, 7]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Choice('l2_reg', values=[0.001, 0.0005, 0.0001]))\n",
    "    )(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Second Conv Layer\n",
    "    x = Conv1D(\n",
    "        filters=hp.Int('conv2_filters', min_value=32, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv2_kernel_size', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Choice('l2_reg', values=[0.001, 0.0005, 0.0001]))\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Third Conv Layer (New)\n",
    "    x = Conv1D(\n",
    "        filters=hp.Int('conv3_filters', min_value=64, max_value=128, step=32),\n",
    "        kernel_size=hp.Choice('conv3_kernel_size', values=[3, 5]),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Choice('l2_reg', values=[0.001, 0.0005, 0.0001]))\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "    # Dense Layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(\n",
    "        units=hp.Int('dense_units', min_value=64, max_value=256, step=64),\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hp.Choice('l2_reg', values=[0.001, 0.0005, 0.0001]))\n",
    "    )(x)\n",
    "    \n",
    "    # Dropout Layer\n",
    "    x = Dropout(rate=hp.Float('dropout_rate', min_value=0.3, max_value=0.7, step=0.1))(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    output_layer = Dense(len(label_mapping), activation='softmax')(x)\n",
    "\n",
    "    # Learning Rate Tuning\n",
    "    learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss='categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Initialize Keras Tuner for hyperparameter search with the updated model\n",
    "tuner = kt.RandomSearch(\n",
    "    build_model,\n",
    "    objective='val_accuracy',\n",
    "    max_trials=20,  # Number of hyperparameter combinations to try\n",
    "    executions_per_trial=1,\n",
    "    directory='tuner_results',\n",
    "    project_name='cnn_rul_tuning_with_third_layer'\n",
    ")\n",
    "\n",
    "# Display the search space summary\n",
    "tuner.search_space_summary()\n",
    "\n",
    "# Run the tuner search with increased epochs and early stopping patience\n",
    "tuner.search(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # Increased from 50 to 100 epochs\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test),\n",
    "    callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=30)]\n",
    ")\n",
    "\n",
    "# Display the best hyperparameters after tuning\n",
    "tuner.results_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the best hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "# Display best hyperparameters (for confirmation)\n",
    "print(f\"\"\"\n",
    "Best Hyperparameters:\n",
    "- conv1_filters: {best_hps.get('conv1_filters')}\n",
    "- conv1_kernel_size: {best_hps.get('conv1_kernel_size')}\n",
    "- l2_reg: {best_hps.get('l2_reg')}\n",
    "- conv2_filters: {best_hps.get('conv2_filters')}\n",
    "- conv2_kernel_size: {best_hps.get('conv2_kernel_size')}\n",
    "- dense_units: {best_hps.get('dense_units')}\n",
    "- dropout_rate: {best_hps.get('dropout_rate')}\n",
    "- learning_rate: {best_hps.get('learning_rate')}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = {\n",
    "    'conv1_filters': 32,\n",
    "    'conv1_kernel_size': 7,\n",
    "    'l2_reg': 0.0005,\n",
    "    'conv2_filters': 32,\n",
    "    'conv2_kernel_size': 5,\n",
    "    'dense_units': 256,\n",
    "    'dropout_rate': 0.4,\n",
    "    'learning_rate': 0.0001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model_from_hps(hps):\n",
    "    input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "    \n",
    "    # First Conv Layer\n",
    "    x = Conv1D(\n",
    "        filters=hps['conv1_filters'],\n",
    "        kernel_size=hps['conv1_kernel_size'],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hps['l2_reg'])\n",
    "    )(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Second Conv Layer\n",
    "    x = Conv1D(\n",
    "        filters=hps['conv2_filters'],\n",
    "        kernel_size=hps['conv2_kernel_size'],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hps['l2_reg'])\n",
    "    )(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling1D(pool_size=2)(x)\n",
    "    \n",
    "    # Dense Layer\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(\n",
    "        units=hps['dense_units'],\n",
    "        activation='relu',\n",
    "        kernel_regularizer=l2(hps['l2_reg'])\n",
    "    )(x)\n",
    "    \n",
    "    # Dropout Layer\n",
    "    x = Dropout(rate=hps['dropout_rate'])(x)\n",
    "    \n",
    "    # Output Layer\n",
    "    output_layer = Dense(len(label_mapping), activation='softmax')(x)\n",
    "    \n",
    "    # Compile Model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    optimizer = Adam(learning_rate=hps['learning_rate'])\n",
    "    \n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build the model\n",
    "model = build_model_from_hps(best_hps)\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,  # You can adjust epochs as needed\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test))\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Ensure consistent labels even if some classes are missing in y_true/y_pred\n",
    "all_labels = np.arange(len(labels))  # [0, 1, 2, 3, 4]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, labels=all_labels, target_names=labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(None)\n",
    "random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "labels = [\"0-200\", \"200-400\", \">400\"]\n",
    "\n",
    "# Set bins and labels\n",
    "label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare sequences\n",
    "X = np.array(df_filtered[\"History\"].tolist())  # Direct NumPy conversion (avoiding unnecessary padding)\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize sequences\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = np.array(df_train[\"History\"].tolist()), np.array(df_test[\"History\"].tolist())\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize training/testing separately\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_mapping))\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "#class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "#class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define CNN model\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "x = Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_regularizer=l2(0.001))(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.6)(x)  # Increased dropout to reduce overfitting\n",
    "output_layer = Dense(len(label_mapping), activation=\"softmax\")(x)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Use an ExponentialDecay schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Remove ReduceLROnPlateau\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=100,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test),\n",
    "    #class_weight=class_weight_dict,  # Handle class imbalance\n",
    "    #callbacks=[early_stopping]  # Only early stopping\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "\n",
    "# Plot training & validation accuracy\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Val Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot training & validation loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Train Loss')\n",
    "plt.plot(history.history['val_loss'], label='Val Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Predict classes\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Ensure consistent labels even if some classes are missing in y_true/y_pred\n",
    "all_labels = np.arange(len(labels))  # [0, 1, 2, 3, 4]\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(y_true, y_pred, labels=all_labels, target_names=labels, zero_division=0))\n",
    "\n",
    "# Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_true, y_pred, labels=all_labels)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def grad_cam_1d(model, input_sequence, class_index, conv_layer_name=\"conv1d_2\"):\n",
    "    \"\"\"\n",
    "    Computes Grad-CAM heatmap for 1D convolutional layers.\n",
    "    \"\"\"\n",
    "    # Ensure input has batch and channel dimensions\n",
    "    input_sequence = np.expand_dims(input_sequence, axis=(0, -1))  # (1, time_steps, 1)\n",
    "    input_sequence = tf.convert_to_tensor(input_sequence, dtype=tf.float32)\n",
    "\n",
    "    # Get target convolutional layer\n",
    "    conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "    # Create model to extract feature maps and predictions\n",
    "    grad_model = Model(inputs=model.input, outputs=[conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        conv_output, predictions = grad_model(input_sequence)\n",
    "        class_score = predictions[:, class_index]\n",
    "\n",
    "    # Compute gradients of the class score w.r.t. convolutional output\n",
    "    grads = tape.gradient(class_score, conv_output)\n",
    "    if grads is None or np.isnan(grads.numpy()).any():\n",
    "        print(\"Warning: Gradients contain NaNs! Replacing with zeros.\")\n",
    "        grads = tf.zeros_like(conv_output)\n",
    "\n",
    "    # Compute importance weights\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=1)\n",
    "\n",
    "    # Apply weights to feature maps\n",
    "    conv_output = conv_output[0]  # Remove batch dimension\n",
    "    pooled_grads = pooled_grads[0]  # Remove batch dimension for weights\n",
    "    heatmap = tf.reduce_sum(conv_output * pooled_grads, axis=-1).numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)\n",
    "    heatmap /= (np.max(heatmap) + 1e-6)\n",
    "\n",
    "    # Interpolate heatmap to match input sequence length\n",
    "    input_seq_len = input_sequence.shape[1]\n",
    "    heatmap_len = len(heatmap)\n",
    "\n",
    "    if heatmap_len != input_seq_len:\n",
    "        heatmap_rescaled = np.interp(\n",
    "            np.arange(input_seq_len),\n",
    "            np.linspace(0, input_seq_len - 1, heatmap_len),  # Map heatmap length to input length\n",
    "            heatmap\n",
    "        )\n",
    "    else:\n",
    "        heatmap_rescaled = heatmap  # No interpolation needed if lengths match\n",
    "    \n",
    "    return heatmap_rescaled\n",
    "\n",
    "\n",
    "# Run Grad-CAM on random samples\n",
    "for i in range(4):\n",
    "    sample_idx = np.random.randint(len(X_test))\n",
    "    input_seq = X_test[sample_idx]\n",
    "    true_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "    heatmap = grad_cam_1d(model, input_seq, class_index=true_label)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(input_seq, label=\"Input Sequence\", alpha=0.6)\n",
    "    plt.plot(heatmap, label=\"Grad-CAM Heatmap\", linewidth=2)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.title(f\"Grad-CAM for Class {true_label}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_cam_for_all_classes(model, input_sequence, true_label, class_labels, conv_layer_name=\"conv1d_2\"):\n",
    "    \"\"\"\n",
    "    Plots Grad-CAM heatmaps for all classes for a given input sequence.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        input_sequence: 1D input array.\n",
    "        true_label: The true class label index.\n",
    "        class_labels: List of class names.\n",
    "        conv_layer_name: Name of the convolutional layer for Grad-CAM.\n",
    "    \"\"\"\n",
    "    num_classes = len(class_labels)\n",
    "    plt.figure(figsize=(15, 3 * num_classes))\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        # Compute Grad-CAM heatmap for each class\n",
    "        heatmap = grad_cam_1d(model, input_sequence, class_index=class_idx, conv_layer_name=conv_layer_name)\n",
    "\n",
    "        # Plot input sequence and corresponding Grad-CAM heatmap\n",
    "        plt.subplot(num_classes, 1, class_idx + 1)\n",
    "        plt.plot(input_sequence.squeeze(), label=\"Input Sequence\", alpha=0.6)\n",
    "        plt.plot(heatmap, label=f\"Grad-CAM: {class_labels[class_idx]}\", linewidth=2)\n",
    "        \n",
    "        if class_idx == true_label:\n",
    "            plt.title(f\"Class {class_labels[class_idx]} (True Label)\", color='green', fontsize=12)\n",
    "        else:\n",
    "            plt.title(f\"Class {class_labels[class_idx]}\", fontsize=12)\n",
    "            \n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Activation\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run Grad-CAM for all classes on a random sample\n",
    "sample_idx = np.random.randint(len(X_test))\n",
    "input_seq = X_test[sample_idx]\n",
    "true_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "plot_grad_cam_for_all_classes(model, input_seq, true_label, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_grad_cam_for_all_classes_misclassified(model, X_test, y_test, class_labels, conv_layer_name=\"conv1d_2\"):\n",
    "    \"\"\"\n",
    "    Finds a misclassified sample and plots Grad-CAM heatmaps for all classes.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        X_test: Test data (input sequences).\n",
    "        y_test: True labels (one-hot encoded).\n",
    "        class_labels: List of class names.\n",
    "        conv_layer_name: Name of the convolutional layer for Grad-CAM.\n",
    "    \"\"\"\n",
    "    # Get model predictions\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "    y_true = np.argmax(y_test, axis=1)\n",
    "\n",
    "    # Find indices of misclassified samples\n",
    "    misclassified_indices = np.where(y_pred != y_true)[0]\n",
    "\n",
    "    if len(misclassified_indices) == 0:\n",
    "        print(\"No misclassified samples found!\")\n",
    "        return\n",
    "\n",
    "    # Select a random misclassified sample\n",
    "    sample_idx = np.random.choice(misclassified_indices)\n",
    "    input_seq = X_test[sample_idx]\n",
    "    true_label = y_true[sample_idx]\n",
    "    predicted_label = y_pred[sample_idx]\n",
    "\n",
    "    print(f\"Selected Misclassified Sample Index: {sample_idx}\")\n",
    "    print(f\"True Label: {class_labels[true_label]}\")\n",
    "    print(f\"Predicted Label: {class_labels[predicted_label]}\")\n",
    "\n",
    "    # Plot Grad-CAM for all classes\n",
    "    num_classes = len(class_labels)\n",
    "    plt.figure(figsize=(15, 3 * num_classes))\n",
    "\n",
    "    for class_idx in range(num_classes):\n",
    "        heatmap = grad_cam_1d(model, input_seq, class_index=class_idx, conv_layer_name=conv_layer_name)\n",
    "\n",
    "        plt.subplot(num_classes, 1, class_idx + 1)\n",
    "        plt.plot(input_seq.squeeze(), label=\"Input Sequence\", alpha=0.6)\n",
    "        plt.plot(heatmap, label=f\"Grad-CAM: {class_labels[class_idx]}\", linewidth=2)\n",
    "\n",
    "        # Highlight true and predicted labels\n",
    "        if class_idx == true_label:\n",
    "            plt.title(f\"Class {class_labels[class_idx]} (True Label)\", color='green', fontsize=12)\n",
    "        elif class_idx == predicted_label:\n",
    "            plt.title(f\"Class {class_labels[class_idx]} (Predicted Label)\", color='red', fontsize=12)\n",
    "        else:\n",
    "            plt.title(f\"Class {class_labels[class_idx]}\", fontsize=12)\n",
    "\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"Activation\")\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run Grad-CAM visualization for a misclassified sample\n",
    "plot_grad_cam_for_all_classes_misclassified(model, X_test, y_test, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the Aachen dataset\n",
    "def preprocess_aachen_dataset(\n",
    "    file_path,\n",
    "    test_cell_count=3,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the Aachen dataset for CNN training, with optional phase filtering\n",
    "    and optional log transform of the target values (RUL80).\n",
    "    \"\"\"\n",
    "    data_loader = mpy.loadmat(file_path)\n",
    "    df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "    # Filter to where the last cycle in history is 600\n",
    "    df = df[df[\"History_Cycle\"].apply(lambda x: x[-1]) == 600]\n",
    "\n",
    "    # Compute EOL80 and RUL80 if missing\n",
    "    def compute_eol_and_rul80(row):\n",
    "        history_cap = np.array(row[\"History\"])\n",
    "        history_cycles = np.array(row[\"History_Cycle\"])\n",
    "        target_cap = np.array(row[\"Target_expanded\"])\n",
    "        target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "        eol80_cycle, rul80 = np.nan, np.nan\n",
    "        if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "            return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "        \n",
    "        initial_capacity = history_cap[0]\n",
    "        threshold = 0.8 * initial_capacity\n",
    "\n",
    "        if history_cap[-1] <= threshold:\n",
    "            return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "        \n",
    "        if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "            return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "        below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "        if len(below_threshold_indices) > 0:\n",
    "            eol80_index = below_threshold_indices[0]\n",
    "            eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "        if not pd.isna(eol80_cycle):\n",
    "            last_history_cycle = history_cycles[-1]\n",
    "            rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "    # Extract sequence lengths for analysis\n",
    "    df[\"Sequence_Length\"] = df[\"History\"].apply(len)\n",
    "    \n",
    "    # Train-test split\n",
    "    cells_to_hold_back = df[\"Cell\"].unique()[:test_cell_count]\n",
    "    df_test = df[df[\"Cell\"].isin(cells_to_hold_back)]\n",
    "    df_train_val = df[~df[\"Cell\"].isin(cells_to_hold_back)]\n",
    "\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_train_val, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    history_train = df_train[\"History\"].tolist()\n",
    "    history_val = df_val[\"History\"].tolist()\n",
    "    history_test = df_test[\"History\"].tolist()\n",
    "\n",
    "    y_train = np.array(df_train[\"RUL80\"])\n",
    "    y_val = np.array(df_val[\"RUL80\"])\n",
    "    y_test = np.array(df_test[\"RUL80\"])\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    all_histories = history_train + history_val + history_test\n",
    "    all_histories_flat = np.concatenate(all_histories)\n",
    "    scaler.fit(all_histories_flat.reshape(-1, 1))\n",
    "\n",
    "    history_train_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_train]\n",
    "    history_val_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_val]\n",
    "    history_test_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_test]\n",
    "    \n",
    "    return {\n",
    "        \"X_train\": history_train_norm,\n",
    "        \"X_val\": history_val_norm,\n",
    "        \"X_test\": history_test_norm,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"\n",
    "data = preprocess_aachen_dataset(file_path)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of sequence lengths\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot([len(seq) for seq in data[\"X_train\"]], bins=20, kde=True)\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Sequence Lengths in X_train\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and preprocess the Aachen dataset\n",
    "def preprocess_aachen_dataset(\n",
    "    file_path,\n",
    "    test_cell_count=3,\n",
    "    random_state=42,\n",
    "):\n",
    "    \"\"\"\n",
    "    Preprocess the Aachen dataset for CNN training, with optional phase filtering\n",
    "    and optional log transform of the target values (RUL80).\n",
    "    \"\"\"\n",
    "    data_loader = mpy.loadmat(file_path)\n",
    "    df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "    # Filter to where the last cycle in history is 600\n",
    "    df = df[df[\"History_Cycle\"].apply(lambda x: x[-1]) == 600]\n",
    "\n",
    "    # Compute EOL80 and RUL80 if missing\n",
    "    def compute_eol_and_rul80(row):\n",
    "        history_cap = np.array(row[\"History\"])\n",
    "        history_cycles = np.array(row[\"History_Cycle\"])\n",
    "        target_cap = np.array(row[\"Target_expanded\"])\n",
    "        target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "        eol80_cycle, rul80 = np.nan, np.nan\n",
    "        if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "            return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "        \n",
    "        initial_capacity = history_cap[0]\n",
    "        threshold = 0.8 * initial_capacity\n",
    "\n",
    "        if history_cap[-1] <= threshold:\n",
    "            return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "        \n",
    "        if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "            return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "        below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "        if len(below_threshold_indices) > 0:\n",
    "            eol80_index = below_threshold_indices[0]\n",
    "            eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "        if not pd.isna(eol80_cycle):\n",
    "            last_history_cycle = history_cycles[-1]\n",
    "            rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "    # Extract sequence lengths for analysis\n",
    "    df[\"Sequence_Length\"] = df[\"History\"].apply(len)\n",
    "    \n",
    "    # Bin the RUL80 values into two categories\n",
    "    bins = [0, 600, np.inf]\n",
    "    labels = [0, 1]  # Categories for bins\n",
    "    df[\"RUL80_binned\"] = pd.cut(df[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n",
    "    \n",
    "    # Train-test split\n",
    "    cells_to_hold_back = df[\"Cell\"].unique()[:test_cell_count]\n",
    "    df_test = df[df[\"Cell\"].isin(cells_to_hold_back)]\n",
    "    df_train_val = df[~df[\"Cell\"].isin(cells_to_hold_back)]\n",
    "\n",
    "    df_train, df_val = train_test_split(\n",
    "        df_train_val, test_size=0.2, random_state=random_state\n",
    "    )\n",
    "\n",
    "    history_train = df_train[\"History\"].tolist()\n",
    "    history_val = df_val[\"History\"].tolist()\n",
    "    history_test = df_test[\"History\"].tolist()\n",
    "\n",
    "    y_train = np.array(df_train[\"RUL80_binned\"].astype(int))\n",
    "    y_val = np.array(df_val[\"RUL80_binned\"].astype(int))\n",
    "    y_test = np.array(df_test[\"RUL80_binned\"].astype(int))\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    all_histories = history_train + history_val + history_test\n",
    "    all_histories_flat = np.concatenate(all_histories)\n",
    "    scaler.fit(all_histories_flat.reshape(-1, 1))\n",
    "\n",
    "    history_train_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_train]\n",
    "    history_val_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_val]\n",
    "    history_test_norm = [scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_test]\n",
    "    \n",
    "    return {\n",
    "        \"X_train\": history_train_norm,\n",
    "        \"X_val\": history_val_norm,\n",
    "        \"X_test\": history_test_norm,\n",
    "        \"y_train\": y_train,\n",
    "        \"y_val\": y_val,\n",
    "        \"y_test\": y_test\n",
    "    }\n",
    "\n",
    "# Load data\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"\n",
    "data = preprocess_aachen_dataset(file_path)\n",
    "\n",
    "# Plot all sequences in training data\n",
    "plt.figure(figsize=(12, 6))\n",
    "for seq in data[\"X_train\"]:\n",
    "    plt.plot(seq, alpha=0.3)\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Normalized Capacity\")\n",
    "plt.title(\"All Training Sequences\")\n",
    "plt.show()\n",
    "\n",
    "# Plot histogram of sequence lengths\n",
    "sequence_lengths = [len(seq) for seq in data[\"X_train\"]]\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(sequence_lengths, bins=20, kde=True)\n",
    "plt.xlabel(\"Sequence Length\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Sequence Lengths in Dataset\")\n",
    "plt.show()\n",
    "\n",
    "# Plot distribution of binned RUL values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(data[\"y_train\"], bins=2, discrete=True)\n",
    "plt.xlabel(\"RUL Bin\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Binned RUL Values in Training Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"  # Adjust if needed\n",
    "data_loader = mpy.loadmat(file_path)\n",
    "df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "# Filter to where the last cycle in history is exactly 600\n",
    "df = df[df[\"History_Cycle\"].apply(lambda x: x[-1]) == 600]\n",
    "\n",
    "# Compute EOL80 and RUL80 if missing\n",
    "def compute_eol_and_rul80(row):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "    target_cap = np.array(row[\"Target_expanded\"])\n",
    "    target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "    eol80_cycle, rul80 = np.nan, np.nan\n",
    "    if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "    \n",
    "    initial_capacity = history_cap[0]\n",
    "    threshold = 0.8 * initial_capacity\n",
    "\n",
    "    if history_cap[-1] <= threshold:\n",
    "        return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "    \n",
    "    if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "    if len(below_threshold_indices) > 0:\n",
    "        eol80_index = below_threshold_indices[0]\n",
    "        eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "    if not pd.isna(eol80_cycle):\n",
    "        last_history_cycle = history_cycles[-1]\n",
    "        rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "    return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "# Drop NaN values for RUL80 before binning\n",
    "df_filtered = df.dropna(subset=[\"RUL80\"])\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 400, 500, 600, np.inf]\n",
    "labels = [\"<400\", \"400-500\", \"500-600\", \">600\"]\n",
    "\n",
    "# Apply binning\n",
    "df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Plot histogram of binned RUL80 values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_filtered[\"RUL80_binned\"], discrete=True, shrink=0.8)\n",
    "plt.xlabel(\"Binned RUL80 (Cycles)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Binned RUL80 Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(None)  # Ensures different results across script executions\n",
    "random.seed(None)  # Also resets Pythons built-in random module\n",
    "tf.random.set_seed(None)  # Ensures different TensorFlow random operations\n",
    "\n",
    "\n",
    "# Convert RUL bins to integer labels\n",
    "label_mapping = {\"<400\": 0, \"400-500\": 1, \"500-600\": 2, \">600\": 3}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare history sequences\n",
    "X = df_filtered[\"History\"].tolist()\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_seq_length = max(len(seq) for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "\n",
    "# Normalize the sequences\n",
    "scaler = MinMaxScaler()\n",
    "X_padded = scaler.fit_transform(X_padded.reshape(-1, 1)).reshape(X_padded.shape)\n",
    "\n",
    "# Debug: Check for NaNs in input\n",
    "print(\"Max input value:\", np.max(X_padded))\n",
    "print(\"Min input value:\", np.min(X_padded))\n",
    "print(\"Any NaNs?:\", np.isnan(X_padded).any())\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = df_train[\"History\"].tolist(), df_test[\"History\"].tolist()\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Pad and normalize new splits\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=4)\n",
    "y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "# Define CNN model\n",
    "input_layer = Input(shape=(max_seq_length, 1))\n",
    "x = Conv1D(filters=32, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_1\")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(4, activation=\"softmax\")(x)  # Changed back to softmax\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile with lower learning rate and gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0)  # Reduce LR\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "def grad_cam_1d(model, input_sequence, class_index, conv_layer_name=\"conv1d_2\"):\n",
    "    input_sequence = np.expand_dims(input_sequence, axis=0)  # Add batch dimension\n",
    "    input_sequence = tf.convert_to_tensor(input_sequence, dtype=tf.float32)  # Convert to tensor\n",
    "    \n",
    "    conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "    # Create model to extract activations & logits\n",
    "    grad_model = Model(inputs=model.input, outputs=[conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        conv_output, predictions = grad_model(input_sequence)\n",
    "        predicted_class = predictions[:, class_index]  # Use logits instead of softmax\n",
    "\n",
    "    grads = tape.gradient(predicted_class, conv_output)\n",
    "\n",
    "    if grads is None or np.isnan(grads.numpy()).any():\n",
    "        print(\"Warning: Gradients contain NaNs! Replacing with zeros.\")\n",
    "        grads = tf.zeros_like(conv_output)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))  # Mean over time and filters\n",
    "    conv_output = conv_output[0]  # Remove batch dimension\n",
    "\n",
    "    # Weighted sum of activations\n",
    "    heatmap = tf.reduce_mean(conv_output * pooled_grads, axis=-1).numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap -= np.min(heatmap)\n",
    "    heatmap /= (np.max(heatmap) + 1e-6)\n",
    "\n",
    "    # Rescale to match input sequence length\n",
    "    heatmap_rescaled = np.interp(np.arange(len(input_sequence.numpy().flatten())), \n",
    "                                 np.linspace(0, len(input_sequence.numpy().flatten()) - 1, len(heatmap)), \n",
    "                                 heatmap)\n",
    "    \n",
    "    return heatmap_rescaled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    sample_idx = np.random.randint(len(X_test))\n",
    "    input_seq = X_test[sample_idx]\n",
    "    true_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "    heatmap = grad_cam_1d(model, input_seq, class_index=true_label)\n",
    "\n",
    "    print(heatmap)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(input_seq, label=\"Input Sequence\", alpha=0.6)\n",
    "    plt.plot(heatmap, label=\"Grad-CAM Heatmap\", linewidth=2)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.title(f\"Grad-CAM for Class {true_label}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heatmap min:\", np.min(heatmap))\n",
    "print(\"Heatmap max:\", np.max(heatmap))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"  # Adjust if needed\n",
    "data_loader = mpy.loadmat(file_path)\n",
    "df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "# Function to trim history arrays to the last 120 cycles\n",
    "def trim_history(row, min_length=120):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "\n",
    "    if len(history_cap) < min_length or len(history_cycles) < min_length:\n",
    "        return pd.Series({\"History\": np.nan, \"History_Cycle\": np.nan})\n",
    "\n",
    "    return pd.Series({\n",
    "        \"History\": history_cap[-min_length:],  # Keep last 120 values\n",
    "        \"History_Cycle\": history_cycles[-min_length:]  # Keep last 120 cycles\n",
    "    })\n",
    "\n",
    "# Apply trimming\n",
    "df[[\"History\", \"History_Cycle\"]] = df.apply(trim_history, axis=1)\n",
    "\n",
    "# Drop NaN values (for sequences shorter than 120 cycles)\n",
    "df = df.dropna(subset=[\"History\"])\n",
    "\n",
    "# Compute EOL80 and RUL80\n",
    "def compute_eol_and_rul80(row):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "    target_cap = np.array(row[\"Target_expanded\"])\n",
    "    target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "    eol80_cycle, rul80 = np.nan, np.nan\n",
    "    if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "    \n",
    "    initial_capacity = history_cap[0]\n",
    "    threshold = 0.8 * initial_capacity\n",
    "\n",
    "    if history_cap[-1] <= threshold:\n",
    "        return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "    \n",
    "    if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "    if len(below_threshold_indices) > 0:\n",
    "        eol80_index = below_threshold_indices[0]\n",
    "        eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "    if not pd.isna(eol80_cycle):\n",
    "        last_history_cycle = history_cycles[-1]\n",
    "        rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "    return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "# Drop NaN values for RUL80 before binning\n",
    "df_filtered = df.dropna(subset=[\"RUL80\"])\n",
    "\n",
    "# Define bins and labels\n",
    "bins = [0, 400, 500, 600, np.inf]\n",
    "labels = [\"<400\", \"400-500\", \"500-600\", \">600\"]\n",
    "\n",
    "# Define bins and labesl, but with categories starting from 0-100 and increasing by 100\n",
    "bins = [0, 100, 200, 300, 400, 500, 600, np.inf]\n",
    "labels = [\"0-100\", \"100-200\", \"200-300\", \"300-400\", \"400-500\", \"500-600\", \">600\"]\n",
    "\n",
    "\n",
    "# Apply binning\n",
    "df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Plot histogram of binned RUL80 values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_filtered[\"RUL80_binned\"], discrete=True, shrink=0.8)\n",
    "plt.xlabel(\"Binned RUL80 (Cycles)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Binned RUL80 Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "np.random.seed(None)  # Ensures different results across script executions\n",
    "random.seed(None)  # Also resets Pythons built-in random module\n",
    "tf.random.set_seed(None)  # Ensures different TensorFlow random operations\n",
    "\n",
    "\n",
    "# Convert RUL bins to integer labels\n",
    "label_mapping = {\"<400\": 0, \"400-500\": 1, \"500-600\": 2, \">600\": 3}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare history sequences\n",
    "X = df_filtered[\"History\"].tolist()\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Pad sequences to the same length\n",
    "max_seq_length = max(len(seq) for seq in X)\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "\n",
    "# Normalize the sequences\n",
    "scaler = MinMaxScaler()\n",
    "X_padded = scaler.fit_transform(X_padded.reshape(-1, 1)).reshape(X_padded.shape)\n",
    "\n",
    "# Debug: Check for NaNs in input\n",
    "print(\"Max input value:\", np.max(X_padded))\n",
    "print(\"Min input value:\", np.min(X_padded))\n",
    "print(\"Any NaNs?:\", np.isnan(X_padded).any())\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=4)\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = df_train[\"History\"].tolist(), df_test[\"History\"].tolist()\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Pad and normalize new splits\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=4)\n",
    "y_test = to_categorical(y_test, num_classes=4)\n",
    "\n",
    "# Define CNN model\n",
    "input_layer = Input(shape=(max_seq_length, 1))\n",
    "x = Conv1D(filters=32, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_1\")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(4, activation=\"softmax\")(x)  # Changed back to softmax\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile with lower learning rate and gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5, clipnorm=1.0)  # Reduce LR\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "def grad_cam_1d(model, input_sequence, class_index, conv_layer_name=\"conv1d_2\"):\n",
    "    input_sequence = np.expand_dims(input_sequence, axis=0)  # Add batch dimension\n",
    "    input_sequence = tf.convert_to_tensor(input_sequence, dtype=tf.float32)  # Convert to tensor\n",
    "    \n",
    "    conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "    # Create model to extract activations & logits\n",
    "    grad_model = Model(inputs=model.input, outputs=[conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(input_sequence)\n",
    "        conv_output, predictions = grad_model(input_sequence)\n",
    "        predicted_class = predictions[:, class_index]  # Use logits instead of softmax\n",
    "\n",
    "    grads = tape.gradient(predicted_class, conv_output)\n",
    "\n",
    "    if grads is None or np.isnan(grads.numpy()).any():\n",
    "        print(\"Warning: Gradients contain NaNs! Replacing with zeros.\")\n",
    "        grads = tf.zeros_like(conv_output)\n",
    "\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1))  # Mean over time and filters\n",
    "    conv_output = conv_output[0]  # Remove batch dimension\n",
    "\n",
    "    # Weighted sum of activations\n",
    "    heatmap = tf.reduce_mean(conv_output * pooled_grads, axis=-1).numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap -= np.min(heatmap)\n",
    "    heatmap /= (np.max(heatmap) + 1e-6)\n",
    "\n",
    "    # Rescale to match input sequence length\n",
    "    heatmap_rescaled = np.interp(np.arange(len(input_sequence.numpy().flatten())), \n",
    "                                 np.linspace(0, len(input_sequence.numpy().flatten()) - 1, len(heatmap)), \n",
    "                                 heatmap)\n",
    "    \n",
    "    return heatmap_rescaled\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    sample_idx = np.random.randint(len(X_test))\n",
    "    input_seq = X_test[sample_idx]\n",
    "    true_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "    heatmap = grad_cam_1d(model, input_seq, class_index=true_label)\n",
    "\n",
    "    print(heatmap)\n",
    "\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(input_seq, label=\"Input Sequence\", alpha=0.6)\n",
    "    plt.plot(heatmap, label=\"Grad-CAM Heatmap\", linewidth=2)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.title(f\"Grad-CAM for Class {true_label}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Nytt forsk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/845637574.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import mat4py as mpy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "file_path = \"Degradation_Prediction_Dataset_ISEA.mat\"  # Adjust if needed\n",
    "data_loader = mpy.loadmat(file_path)\n",
    "df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "# Function to trim history arrays to the last 120 cycles\n",
    "def trim_history(row, min_length=120):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "\n",
    "    if len(history_cap) < min_length or len(history_cycles) < min_length:\n",
    "        return pd.Series({\"History\": np.nan, \"History_Cycle\": np.nan})\n",
    "\n",
    "    return pd.Series({\n",
    "        \"History\": history_cap[-min_length:],  # Keep last 120 values\n",
    "        \"History_Cycle\": history_cycles[-min_length:]  # Keep last 120 cycles\n",
    "    })\n",
    "\n",
    "# Apply trimming\n",
    "df[[\"History\", \"History_Cycle\"]] = df.apply(trim_history, axis=1)\n",
    "\n",
    "# Drop NaN values (for sequences shorter than 120 cycles)\n",
    "df = df.dropna(subset=[\"History\"])\n",
    "\n",
    "# Compute EOL80 and RUL80\n",
    "def compute_eol_and_rul80(row):\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "    target_cap = np.array(row[\"Target_expanded\"])\n",
    "    target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "    eol80_cycle, rul80 = np.nan, np.nan\n",
    "    if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "    \n",
    "    initial_capacity = history_cap[0]\n",
    "    threshold = 0.8 * initial_capacity\n",
    "\n",
    "    if history_cap[-1] <= threshold:\n",
    "        return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "    \n",
    "    if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "    if len(below_threshold_indices) > 0:\n",
    "        eol80_index = below_threshold_indices[0]\n",
    "        eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "    if not pd.isna(eol80_cycle):\n",
    "        last_history_cycle = history_cycles[-1]\n",
    "        rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "    return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "# Drop NaN values for RUL80 before binning\n",
    "df_filtered = df.dropna(subset=[\"RUL80\"])\n",
    "\n",
    "\n",
    "# Define bins and labesl, but with categories starting from 0-200 and increasing by 200\n",
    "bins = [0, 200, 400, 600, 800, np.inf]\n",
    "labels = [\"0-200\", \"200-400\", \"400-600\", \"600-800\", \">800\"]\n",
    "\n",
    "\n",
    "# Apply binning\n",
    "df_filtered[\"RUL80_binned\"] = pd.cut(df_filtered[\"RUL80\"], bins=bins, labels=labels, include_lowest=True)\n",
    "\n",
    "# Plot histogram of binned RUL80 values\n",
    "plt.figure(figsize=(8, 4))\n",
    "sns.histplot(df_filtered[\"RUL80_binned\"], discrete=True, shrink=0.8)\n",
    "plt.xlabel(\"Binned RUL80 (Cycles)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of Binned RUL80 Categories\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/1930579377.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6285 - loss: 1.2000 - val_accuracy: 0.4198 - val_loss: 1.8650\n",
      "Epoch 2/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7223 - loss: 0.7845 - val_accuracy: 0.6517 - val_loss: 1.1987\n",
      "Epoch 3/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7593 - loss: 0.6639 - val_accuracy: 0.6203 - val_loss: 1.2949\n",
      "Epoch 4/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7596 - loss: 0.6401 - val_accuracy: 0.6195 - val_loss: 0.9447\n",
      "Epoch 5/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7727 - loss: 0.5983 - val_accuracy: 0.6808 - val_loss: 0.8612\n",
      "Epoch 6/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7696 - loss: 0.5937 - val_accuracy: 0.6502 - val_loss: 0.9599\n",
      "Epoch 7/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7817 - loss: 0.5699 - val_accuracy: 0.6981 - val_loss: 0.7877\n",
      "Epoch 8/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7843 - loss: 0.5575 - val_accuracy: 0.6942 - val_loss: 0.7316\n",
      "Epoch 9/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8026 - loss: 0.5430 - val_accuracy: 0.6698 - val_loss: 1.0236\n",
      "Epoch 10/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7875 - loss: 0.5833 - val_accuracy: 0.6769 - val_loss: 1.1440\n",
      "Epoch 11/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8106 - loss: 0.5281 - val_accuracy: 0.7123 - val_loss: 0.8814\n",
      "Epoch 12/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8144 - loss: 0.5180 - val_accuracy: 0.6981 - val_loss: 0.7767\n",
      "Epoch 13/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8292 - loss: 0.4851 - val_accuracy: 0.7752 - val_loss: 0.5186\n",
      "Epoch 14/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.4897 - val_accuracy: 0.7225 - val_loss: 0.7612\n",
      "Epoch 15/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8180 - loss: 0.4845 - val_accuracy: 0.7626 - val_loss: 0.6607\n",
      "Epoch 16/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8149 - loss: 0.4901 - val_accuracy: 0.6785 - val_loss: 0.9640\n",
      "Epoch 17/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8196 - loss: 0.5093 - val_accuracy: 0.6124 - val_loss: 1.0683\n",
      "Epoch 18/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8236 - loss: 0.4841 - val_accuracy: 0.5487 - val_loss: 1.0696\n",
      "Epoch 19/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8287 - loss: 0.4979 - val_accuracy: 0.6643 - val_loss: 1.5906\n",
      "Epoch 20/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8264 - loss: 0.4584 - val_accuracy: 0.6525 - val_loss: 3.8239\n",
      "Epoch 21/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8272 - loss: 0.4634 - val_accuracy: 0.4363 - val_loss: 2.6107\n",
      "Epoch 22/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8337 - loss: 0.4481 - val_accuracy: 0.6250 - val_loss: 3.8162\n",
      "Epoch 23/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8246 - loss: 0.4674 - val_accuracy: 0.6824 - val_loss: 0.7692\n",
      "Epoch 24/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8187 - loss: 0.4929 - val_accuracy: 0.6879 - val_loss: 0.7382\n",
      "Epoch 25/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8238 - loss: 0.4806 - val_accuracy: 0.7358 - val_loss: 0.6110\n",
      "Epoch 26/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8284 - loss: 0.4715 - val_accuracy: 0.7571 - val_loss: 0.5160\n",
      "Epoch 27/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8368 - loss: 0.4499 - val_accuracy: 0.7131 - val_loss: 0.6161\n",
      "Epoch 28/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8227 - loss: 0.4679 - val_accuracy: 0.6266 - val_loss: 1.1387\n",
      "Epoch 29/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8402 - loss: 0.4603 - val_accuracy: 0.7044 - val_loss: 0.7444\n",
      "Epoch 30/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8130 - loss: 0.4813 - val_accuracy: 0.7366 - val_loss: 0.5928\n",
      "Epoch 31/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8209 - loss: 0.5013 - val_accuracy: 0.7178 - val_loss: 0.7280\n",
      "Epoch 32/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8183 - loss: 0.4871 - val_accuracy: 0.6203 - val_loss: 3.6724\n",
      "Epoch 33/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8132 - loss: 0.4759 - val_accuracy: 0.5621 - val_loss: 1.0404\n",
      "Epoch 34/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8322 - loss: 0.4563 - val_accuracy: 0.6525 - val_loss: 4.3350\n",
      "Epoch 35/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8233 - loss: 0.4828 - val_accuracy: 0.7759 - val_loss: 0.5396\n",
      "Epoch 36/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8205 - loss: 0.4717 - val_accuracy: 0.7272 - val_loss: 0.6745\n",
      "Epoch 37/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8212 - loss: 0.4730 - val_accuracy: 0.6942 - val_loss: 0.9699\n",
      "Epoch 38/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8241 - loss: 0.4762 - val_accuracy: 0.7657 - val_loss: 0.5256\n",
      "Epoch 39/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8253 - loss: 0.4723 - val_accuracy: 0.7005 - val_loss: 0.8013\n",
      "Epoch 40/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8276 - loss: 0.4423 - val_accuracy: 0.6895 - val_loss: 0.8369\n",
      "Epoch 41/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8182 - loss: 0.5198 - val_accuracy: 0.7642 - val_loss: 0.5859\n",
      "Epoch 42/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8128 - loss: 0.4872 - val_accuracy: 0.7209 - val_loss: 0.6696\n",
      "Epoch 43/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8257 - loss: 0.4702 - val_accuracy: 0.7115 - val_loss: 0.6161\n",
      "Epoch 44/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8239 - loss: 0.5048 - val_accuracy: 0.4599 - val_loss: 1.5394\n",
      "Epoch 45/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8235 - loss: 0.4960 - val_accuracy: 0.3082 - val_loss: 19.3546\n",
      "Epoch 46/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8393 - loss: 0.4576 - val_accuracy: 0.7704 - val_loss: 0.5893\n",
      "Epoch 47/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8240 - loss: 0.4861 - val_accuracy: 0.6627 - val_loss: 1.5606\n",
      "Epoch 48/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8036 - loss: 0.5346 - val_accuracy: 0.6785 - val_loss: 0.9700\n",
      "Epoch 49/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8041 - loss: 0.5189 - val_accuracy: 0.6706 - val_loss: 1.6275\n",
      "Epoch 50/50\n",
      "\u001b[1m593/593\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8103 - loss: 0.5141 - val_accuracy: 0.7343 - val_loss: 0.5647\n",
      "\u001b[1m40/40\u001b[0m \u001b[32m\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6313 - loss: 0.7713\n",
      "Test Accuracy: 0.7343\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(None)\n",
    "random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "labels = [\"0-200\", \"200-400\", \"400-600\", \"600-800\", \">800\"]\n",
    "\n",
    "# Set bins and labels\n",
    "label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Update\n",
    "\n",
    "# Prepare sequences\n",
    "X = df_filtered[\"History\"].tolist()\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Pad sequences\n",
    "max_seq_length = 120  # Since history is trimmed to last 120 cycles\n",
    "X_padded = pad_sequences(X, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "\n",
    "# Normalize sequences\n",
    "scaler = MinMaxScaler()\n",
    "X_padded = scaler.fit_transform(X_padded.reshape(-1, 1)).reshape(X_padded.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = df_train[\"History\"].tolist(), df_test[\"History\"].tolist()\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Pad and normalize\n",
    "X_train = pad_sequences(X_train, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_test = pad_sequences(X_test, maxlen=max_seq_length, padding='post', dtype=\"float32\")\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_mapping))\n",
    "\n",
    "# Define CNN model\n",
    "input_layer = Input(shape=(max_seq_length, 1))\n",
    "x = Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_1\")(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001), name=\"conv1d_2\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(64, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "output_layer = Dense(len(label_mapping), activation=\"softmax\")(x)  # Adjusted for new categories\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile with lower learning rate and gradient clipping\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_test, y_test)\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nn/4_r89tj92wd44cypgj5z6vkh0000gn/T/ipykernel_45943/3132690001.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
      "/Users/johannesherstad/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/numerical_utils.py:87: RuntimeWarning: invalid value encountered in cast\n",
      "  x = np.array(x, dtype=\"int64\")\n",
      "/Users/johannesherstad/miniconda3/envs/D2D_env/lib/python3.11/site-packages/keras/src/utils/numerical_utils.py:87: RuntimeWarning: invalid value encountered in cast\n",
      "  x = np.array(x, dtype=\"int64\")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_val' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 99\u001b[0m\n\u001b[1;32m     91\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39moptimizer, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# Remove ReduceLROnPlateau\u001b[39;00m\n\u001b[1;32m     95\u001b[0m history \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     96\u001b[0m     X_train, y_train,\n\u001b[1;32m     97\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     98\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m---> 99\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(\u001b[43mX_val\u001b[49m ,y_val),\n\u001b[1;32m    100\u001b[0m     \u001b[38;5;66;03m#class_weight=class_weight_dict,  # Handle class imbalance\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;66;03m#callbacks=[early_stopping]  # Only early stopping\u001b[39;00m\n\u001b[1;32m    102\u001b[0m )\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# Evaluate\u001b[39;00m\n\u001b[1;32m    107\u001b[0m test_loss, test_acc \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_val' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import random\n",
    "\n",
    "# Ensure reproducibility\n",
    "np.random.seed(None)\n",
    "random.seed(None)\n",
    "tf.random.set_seed(None)\n",
    "\n",
    "labels = [\"0-200\", \"200-400\", \"400-600\", \"600-800\", \">800\"]\n",
    "\n",
    "# Set bins and labels\n",
    "label_mapping = {label: i for i, label in enumerate(labels)}\n",
    "df_filtered[\"RUL80_binned\"] = df_filtered[\"RUL80_binned\"].map(label_mapping)\n",
    "\n",
    "# Prepare sequences\n",
    "X = np.array(df_filtered[\"History\"].tolist())  # Direct NumPy conversion (avoiding unnecessary padding)\n",
    "y = np.array(df_filtered[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize sequences\n",
    "scaler = MinMaxScaler()\n",
    "X = scaler.fit_transform(X.reshape(-1, 1)).reshape(X.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_one_hot = to_categorical(y, num_classes=len(label_mapping))\n",
    "\n",
    "# Train-test split ensuring entire cells are separated\n",
    "unique_cells = df_filtered[\"Cell\"].unique()\n",
    "np.random.shuffle(unique_cells)\n",
    "split_ratio = 0.8\n",
    "split_index = int(len(unique_cells) * split_ratio)\n",
    "train_cells = unique_cells[:split_index]\n",
    "test_cells = unique_cells[split_index:]\n",
    "\n",
    "df_train = df_filtered[df_filtered[\"Cell\"].isin(train_cells)]\n",
    "df_test = df_filtered[df_filtered[\"Cell\"].isin(test_cells)]\n",
    "\n",
    "X_train, X_test = np.array(df_train[\"History\"].tolist()), np.array(df_test[\"History\"].tolist())\n",
    "y_train, y_test = np.array(df_train[\"RUL80_binned\"]), np.array(df_test[\"RUL80_binned\"])\n",
    "\n",
    "# Normalize training/testing separately\n",
    "X_train = scaler.transform(X_train.reshape(-1, 1)).reshape(X_train.shape)\n",
    "X_test = scaler.transform(X_test.reshape(-1, 1)).reshape(X_test.shape)\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, num_classes=len(label_mapping))\n",
    "y_test = to_categorical(y_test, num_classes=len(label_mapping))\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "#class_weights = compute_class_weight('balanced', classes=np.unique(y), y=y)\n",
    "#class_weight_dict = dict(enumerate(class_weights))\n",
    "\n",
    "# Define CNN model\n",
    "input_layer = Input(shape=(X_train.shape[1], 1))\n",
    "x = Conv1D(filters=32, kernel_size=5, activation=\"relu\", kernel_regularizer=l2(0.001))(input_layer)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=64, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Conv1D(filters=128, kernel_size=3, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = MaxPooling1D(pool_size=2)(x)\n",
    "\n",
    "x = Flatten()(x)\n",
    "x = Dense(128, activation=\"relu\", kernel_regularizer=l2(0.001))(x)\n",
    "x = Dropout(0.6)(x)  # Increased dropout to reduce overfitting\n",
    "output_layer = Dense(len(label_mapping), activation=\"softmax\")(x)\n",
    "\n",
    "# Create Model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Use an ExponentialDecay schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=1e-3,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9,\n",
    "    staircase=True\n",
    ")\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule, clipnorm=1.0)\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Remove ReduceLROnPlateau\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=50,\n",
    "    batch_size=8,\n",
    "    validation_data=(X_val ,y_val),\n",
    "    #class_weight=class_weight_dict,  # Handle class imbalance\n",
    "    #callbacks=[early_stopping]  # Only early stopping\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "fp and xp are not of the same length.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 68\u001b[0m\n\u001b[1;32m     65\u001b[0m input_seq \u001b[38;5;241m=\u001b[39m X_test[sample_idx]\n\u001b[1;32m     66\u001b[0m true_label \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_test[sample_idx])\n\u001b[0;32m---> 68\u001b[0m heatmap \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_cam_1d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_seq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrue_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Plot results\u001b[39;00m\n\u001b[1;32m     71\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m4\u001b[39m))\n",
      "Cell \u001b[0;32mIn[8], line 54\u001b[0m, in \u001b[0;36mgrad_cam_1d\u001b[0;34m(model, input_sequence, class_index, conv_layer_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Fix shape issue in interpolation\u001b[39;00m\n\u001b[1;32m     53\u001b[0m input_seq_len \u001b[38;5;241m=\u001b[39m input_sequence\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]  \u001b[38;5;66;03m# Get original sequence length\u001b[39;00m\n\u001b[0;32m---> 54\u001b[0m heatmap_rescaled \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterp\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_seq_len\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure correct x-axis values\u001b[39;49;00m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_seq_len\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mheatmap\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match original length\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheatmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Ensure it's a 1D array\u001b[39;49;00m\n\u001b[1;32m     58\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m heatmap_rescaled\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/numpy/lib/function_base.py:1599\u001b[0m, in \u001b[0;36minterp\u001b[0;34m(x, xp, fp, left, right, period)\u001b[0m\n\u001b[1;32m   1596\u001b[0m     xp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((xp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m-\u001b[39mperiod, xp, xp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39mperiod))\n\u001b[1;32m   1597\u001b[0m     fp \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((fp[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:], fp, fp[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m-> 1599\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minterp_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: fp and xp are not of the same length."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def grad_cam_1d(model, input_sequence, class_index, conv_layer_name=\"conv1d_5\"):\n",
    "    \"\"\"\n",
    "    Computes Grad-CAM heatmap for 1D convolutional layers.\n",
    "\n",
    "    Args:\n",
    "        model: Trained Keras model.\n",
    "        input_sequence: 1D input array.\n",
    "        class_index: Target class index.\n",
    "        conv_layer_name: Name of the convolutional layer for Grad-CAM.\n",
    "\n",
    "    Returns:\n",
    "        Heatmap aligned with input sequence length.\n",
    "    \"\"\"\n",
    "    # Ensure input has batch and channel dimensions\n",
    "    input_sequence = np.expand_dims(input_sequence, axis=(0, -1))  # Shape: (1, time_steps, 1)\n",
    "    input_sequence = tf.convert_to_tensor(input_sequence, dtype=tf.float32)\n",
    "\n",
    "    # Get target convolutional layer\n",
    "    conv_layer = model.get_layer(conv_layer_name)\n",
    "\n",
    "    # Create model to extract feature maps and predictions\n",
    "    grad_model = Model(inputs=model.input, outputs=[conv_layer.output, model.output])\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_output, predictions = grad_model(input_sequence)\n",
    "        class_score = predictions[:, class_index]  # Extract score for the target class\n",
    "\n",
    "    # Compute gradients of the class score w.r.t. convolutional output\n",
    "    grads = tape.gradient(class_score, conv_output)\n",
    "\n",
    "    if grads is None or np.isnan(grads.numpy()).any():\n",
    "        print(\"Warning: Gradients contain NaNs! Replacing with zeros.\")\n",
    "        grads = tf.zeros_like(conv_output)\n",
    "\n",
    "    # Compute importance weights\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=1)  # Average over time dimension\n",
    "\n",
    "    # Apply weights to feature maps\n",
    "    conv_output = conv_output[0]  # Remove batch dimension\n",
    "    pooled_grads = tf.expand_dims(pooled_grads, axis=0)  # Ensure broadcasting\n",
    "    heatmap = tf.reduce_sum(conv_output * pooled_grads, axis=-1).numpy()\n",
    "\n",
    "    # Normalize heatmap\n",
    "    heatmap = np.maximum(heatmap, 0)  # ReLU to remove negative values\n",
    "    heatmap /= (np.max(heatmap) + 1e-6)  # Normalize to [0,1]\n",
    "\n",
    "    # Fix shape issue in interpolation\n",
    "    input_seq_len = input_sequence.shape[1]  # Get original sequence length\n",
    "    heatmap_rescaled = np.interp(\n",
    "        np.arange(input_seq_len),  # Ensure correct x-axis values\n",
    "        np.linspace(0, input_seq_len - 1, len(heatmap)),  # Match original length\n",
    "        heatmap.flatten()  # Ensure it's a 1D array\n",
    "    )\n",
    "    \n",
    "    return heatmap_rescaled\n",
    "\n",
    "# Run Grad-CAM on random samples\n",
    "for i in range(4):\n",
    "    sample_idx = np.random.randint(len(X_test))\n",
    "    input_seq = X_test[sample_idx]\n",
    "    true_label = np.argmax(y_test[sample_idx])\n",
    "\n",
    "    heatmap = grad_cam_1d(model, input_seq, class_index=true_label)\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(input_seq, label=\"Input Sequence\", alpha=0.6)\n",
    "    plt.plot(heatmap, label=\"Grad-CAM Heatmap\", linewidth=2)\n",
    "    plt.xlabel(\"Time Step\")\n",
    "    plt.ylabel(\"Normalized Value\")\n",
    "    plt.title(f\"Grad-CAM for Class {true_label}\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
