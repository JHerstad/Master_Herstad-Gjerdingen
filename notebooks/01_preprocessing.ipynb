{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aachen dataset\n",
    "\n",
    "# Set root directory of the project as the current working directory\n",
    "import os\n",
    "initial_dir = os.getcwd()  # Save initial directory (notebooks/)\n",
    "os.chdir('..')  # Move to project/\n",
    "\n",
    "# Import Config from config.defaults and preprocess_aachen_dataset from src.preprocessing\n",
    "from config.defaults import Config\n",
    "from src.preprocessing import preprocess_aachen_dataset\n",
    "\n",
    "# Load the default configuration\n",
    "config = Config()\n",
    "\n",
    "# Preprocess the dataset for regression\n",
    "preprocess_aachen_dataset(config)\n",
    "\n",
    "# Preprocess the dataset for classification\n",
    "config.classification = True \n",
    "preprocess_aachen_dataset(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: data/MIT_Stanford/raw/batch1.pkl not found!\n",
      "Warning: data/MIT_Stanford/raw/batch2.pkl not found!\n",
      "Warning: data/MIT_Stanford/raw/batch3.pkl not found!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by RobustScaler.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Preprocess MIT-Stanford dataset for regression\u001b[39;00m\n\u001b[1;32m     14\u001b[0m config\u001b[38;5;241m.\u001b[39mclassification \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \n\u001b[0;32m---> 15\u001b[0m \u001b[43mpreprocess_mit_stanford_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Preprocess MIT-Stanford dataset for classification\u001b[39;00m\n\u001b[1;32m     18\u001b[0m config\u001b[38;5;241m.\u001b[39mclassification \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/Master_Herstad-Gjerdingen/src/preprocessing.py:410\u001b[0m, in \u001b[0;36mpreprocess_mit_stanford_dataset\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    407\u001b[0m y_expanding \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(exp_ruls)\n\u001b[1;32m    409\u001b[0m \u001b[38;5;66;03m# -------- Step 2: Normalize Input Features --------\u001b[39;00m\n\u001b[0;32m--> 410\u001b[0m scaler_X \u001b[38;5;241m=\u001b[39m RobustScaler()\n\u001b[1;32m    411\u001b[0m X_exp_norm \u001b[38;5;241m=\u001b[39m scaler_X\u001b[38;5;241m.\u001b[39mfit_transform(X_expanding\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(X_expanding\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], max_cycles, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    413\u001b[0m \u001b[38;5;66;03m# -------- Step 3: Train/Test Split by Battery Cells --------\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/utils/_set_output.py:316\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 316\u001b[0m     data_to_wrap \u001b[38;5;241m=\u001b[39m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    317\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    318\u001b[0m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    319\u001b[0m         return_tuple \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    320\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[38;5;241m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    321\u001b[0m             \u001b[38;5;241m*\u001b[39mdata_to_wrap[\u001b[38;5;241m1\u001b[39m:],\n\u001b[1;32m    322\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/base.py:1098\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m   1083\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1084\u001b[0m             (\n\u001b[1;32m   1085\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis object (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) has a `transform`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m             \u001b[38;5;167;01mUserWarning\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m         )\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1097\u001b[0m     \u001b[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m-> 1098\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(X)\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1100\u001b[0m     \u001b[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m   1101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/preprocessing/_data.py:1597\u001b[0m, in \u001b[0;36mRobustScaler.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m   1579\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the median and quantiles to be used for scaling.\u001b[39;00m\n\u001b[1;32m   1580\u001b[0m \n\u001b[1;32m   1581\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1593\u001b[0m \u001b[38;5;124;03m    Fitted scaler.\u001b[39;00m\n\u001b[1;32m   1594\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1595\u001b[0m \u001b[38;5;66;03m# at fit, convert sparse matrices to csc for optimized computation of\u001b[39;00m\n\u001b[1;32m   1596\u001b[0m \u001b[38;5;66;03m# the quantiles\u001b[39;00m\n\u001b[0;32m-> 1597\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1598\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1599\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1600\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1601\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1602\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1604\u001b[0m q_min, q_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantile_range\n\u001b[1;32m   1605\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;241m0\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_min \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m q_max \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/base.py:633\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    631\u001b[0m         out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[0;32m--> 633\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n\u001b[1;32m    635\u001b[0m     out \u001b[38;5;241m=\u001b[39m _check_y(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n",
      "File \u001b[0;32m~/miniconda3/envs/D2D_env/lib/python3.11/site-packages/sklearn/utils/validation.py:1087\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1085\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[1;32m   1086\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[0;32m-> 1087\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1088\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1089\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1090\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[1;32m   1091\u001b[0m         )\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m   1094\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 1)) while a minimum of 1 is required by RobustScaler."
     ]
    }
   ],
   "source": [
    "# Set root directory of the project as the current working directory\n",
    "import os\n",
    "initial_dir = os.getcwd()  # Save initial directory (notebooks/)\n",
    "os.chdir('..')  # Move to project root\n",
    "\n",
    "# Import Config and preprocessing functions\n",
    "from config.defaults import Config\n",
    "from src.preprocessing import preprocess_mit_stanford_dataset\n",
    "\n",
    "# Load the default configuration\n",
    "config = Config()\n",
    "\n",
    "# Preprocess MIT-Stanford dataset for regression\n",
    "config.classification = False  \n",
    "preprocess_mit_stanford_dataset(config)\n",
    "\n",
    "# Preprocess MIT-Stanford dataset for classification\n",
    "config.classification = True\n",
    "preprocess_mit_stanford_dataset(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/\n",
      "Checking: data/MIT_Stanford/raw/batch1.pkl -> Exists: False\n",
      "Checking: data/MIT_Stanford/raw/batch2.pkl -> Exists: False\n",
      "Checking: data/MIT_Stanford/raw/batch3.pkl -> Exists: False\n"
     ]
    }
   ],
   "source": [
    "# Set root directory of the project as the current working directory\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    %cd ..\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_dir = \"data/MIT_Stanford/raw/\"\n",
    "batch_files = [\"batch1.pkl\", \"batch2.pkl\", \"batch3.pkl\"]\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    batch_path = os.path.join(data_dir, batch_file)\n",
    "    print(f\"Checking: {batch_path} -> Exists: {os.path.exists(batch_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: /\n",
      "Contents of data/MIT_Stanford/raw/: Directory not found\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print the full absolute path where it is searching\n",
    "print(f\"Current Working Directory: {os.getcwd()}\")\n",
    "\n",
    "# Check what's inside the directory\n",
    "print(f\"Contents of {data_dir}: {os.listdir(data_dir) if os.path.exists(data_dir) else 'Directory not found'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute data directory path: /data/MIT_Stanford/raw\n",
      "Checking: /data/MIT_Stanford/raw/batch1.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch2.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch3.pkl -> Exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_dir = os.path.abspath(\"data/MIT_Stanford/raw/\")\n",
    "print(f\"Absolute data directory path: {data_dir}\")\n",
    "\n",
    "batch_files = [\"batch1.pkl\", \"batch2.pkl\", \"batch3.pkl\"]\n",
    "\n",
    "for batch_file in batch_files:\n",
    "    batch_path = os.path.join(data_dir, batch_file)\n",
    "    print(f\"Checking: {batch_path} -> Exists: {os.path.exists(batch_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /data/MIT_Stanford/raw\n",
      "Checking: /data/MIT_Stanford/raw/batch1.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch2.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch3.pkl -> Exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Automatically find the absolute path of the dataset\n",
    "project_root = os.path.abspath(\"..\")  # Moves one level up from notebooks/\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"MIT_Stanford\", \"raw\")\n",
    "\n",
    "print(f\"Loading dataset from: {raw_data_dir}\")  # Debugging check\n",
    "\n",
    "batch_files = [\"batch1.pkl\", \"batch2.pkl\", \"batch3.pkl\"]\n",
    "for batch_file in batch_files:\n",
    "    batch_path = os.path.join(raw_data_dir, batch_file)\n",
    "    print(f\"Checking: {batch_path} -> Exists: {os.path.exists(batch_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset from: /data/MIT_Stanford/raw\n",
      "Checking: /data/MIT_Stanford/raw/batch1.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch2.pkl -> Exists: False\n",
      "Checking: /data/MIT_Stanford/raw/batch3.pkl -> Exists: False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Set project root dynamically\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # Moves one level up if running from notebooks/\n",
    "\n",
    "# Construct the correct dataset path\n",
    "raw_data_dir = os.path.join(project_root, \"data\", \"MIT_Stanford\", \"raw\")\n",
    "\n",
    "print(f\"Loading dataset from: {raw_data_dir}\")  # Debugging check\n",
    "\n",
    "batch_files = [\"batch1.pkl\", \"batch2.pkl\", \"batch3.pkl\"]\n",
    "for batch_file in batch_files:\n",
    "    batch_path = os.path.join(raw_data_dir, batch_file)\n",
    "    print(f\"Checking: {batch_path} -> Exists: {os.path.exists(batch_path)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
