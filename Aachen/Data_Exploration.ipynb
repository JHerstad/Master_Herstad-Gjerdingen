{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for Data Exploration of Aachen dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import mat4py as mpy\n",
    "\n",
    "from Load_and_Preprocess_Aachen import preprocess_aachen_dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phase = None, test_cell_count = 0 -> All cells are included in the training set to explore the full dataset\n",
    "aachen_data = preprocess_aachen_dataset(\n",
    "    \"/Users/johannesherstad/Master_Herstad-Gjerdingen/Aachen/Degradation_Prediction_Dataset_ISEA.mat\",\n",
    "    test_cell_count=0,\n",
    "    random_state=42,\n",
    "    phase=None,\n",
    "    log_transform=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aachen_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(aachen_data[\"X_train\"].shape, aachen_data[\"y_train\"].shape)\n",
    "print(aachen_data[\"X_val\"].shape, aachen_data[\"y_val\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mat4py as mpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/Users/johannesherstad/Master_Herstad-Gjerdingen/Aachen/Degradation_Prediction_Dataset_ISEA.mat\"\n",
    "test_cell_count=3\n",
    "random_state=42\n",
    "phase=None\n",
    "early_threshold=800\n",
    "mid_threshold=400\n",
    "log_transform=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1) Load and transform the raw data from .mat into a Pandas DataFrame ---\n",
    "data_loader = mpy.loadmat(file_path)\n",
    "df = pd.DataFrame.from_dict(data_loader[\"TDS\"])\n",
    "\n",
    "def compute_eol_and_rul80(row):\n",
    "    \"\"\"\n",
    "    Compute EOL80 (End of Life where capacity drops below 80% of initial)\n",
    "    and RUL80 (Remaining Useful Life at 80% capacity).\n",
    "    \"\"\"\n",
    "    history_cap = np.array(row[\"History\"])\n",
    "    history_cycles = np.array(row[\"History_Cycle\"])\n",
    "    target_cap = np.array(row[\"Target_expanded\"])\n",
    "    target_cycles = np.array(row[\"Target_Cycle_Expanded\"])\n",
    "\n",
    "    eol80_cycle = np.nan\n",
    "    rul80 = np.nan\n",
    "\n",
    "    # Handle cases with missing data in historical capacity or cycles\n",
    "    if len(history_cap) == 0 or len(history_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    # Determine the threshold for EOL80\n",
    "    initial_capacity = history_cap[0]\n",
    "    threshold = 0.8 * initial_capacity\n",
    "\n",
    "    # Check if the historical capacity already falls below the threshold\n",
    "    if history_cap[-1] <= threshold:\n",
    "        return pd.Series({\"EOL80\": np.nan, \"RUL80\": np.nan})\n",
    "\n",
    "    # Handle cases with missing target data\n",
    "    if len(target_cap) == 0 or len(target_cycles) == 0:\n",
    "        return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "    # Find the first cycle where capacity drops below the threshold in the target portion\n",
    "    below_threshold_indices = np.where(target_cap < threshold)[0]\n",
    "    if len(below_threshold_indices) > 0:\n",
    "        eol80_index = below_threshold_indices[0]\n",
    "        eol80_cycle = target_cycles[eol80_index]\n",
    "\n",
    "    # Calculate RUL80 as the difference between EOL80 and the last history cycle\n",
    "    if not pd.isna(eol80_cycle):\n",
    "        last_history_cycle = history_cycles[-1]\n",
    "        rul80 = eol80_cycle - last_history_cycle\n",
    "\n",
    "    return pd.Series({\"EOL80\": eol80_cycle, \"RUL80\": rul80})\n",
    "\n",
    "# Compute EOL80 and RUL80 for each row\n",
    "df[[\"EOL80\", \"RUL80\"]] = df.apply(compute_eol_and_rul80, axis=1)\n",
    "\n",
    "# Filter out rows with invalid RUL80 values (NaN, etc.)\n",
    "df_valid = df[df[\"RUL80\"].notna()]\n",
    "\n",
    "# --- 2) Hold back specific cells for testing ---\n",
    "cells_to_hold_back = df_valid[\"Cell\"].unique()[:test_cell_count]\n",
    "df_test = df_valid[df_valid[\"Cell\"].isin(cells_to_hold_back)]\n",
    "df_train_val = df_valid[~df_valid[\"Cell\"].isin(cells_to_hold_back)]\n",
    "\n",
    "# --- 3) Split the remaining data into training and validation sets ---\n",
    "df_train, df_val = train_test_split(\n",
    "    df_train_val,\n",
    "    test_size=0.2,\n",
    "    random_state=random_state,\n",
    "    stratify=df_train_val[\"Cell\"]\n",
    ")\n",
    "\n",
    "# --- 4) (OPTIONAL) Phase-based filtering for train, val, test ---\n",
    "def filter_by_phase(df_in, phase_name, early_thr, mid_thr):\n",
    "    if phase_name == \"early\":\n",
    "        # Keep samples with RUL80 > early_threshold\n",
    "        return df_in[df_in[\"RUL80\"] > early_thr]\n",
    "    elif phase_name == \"mid\":\n",
    "        # Keep samples where mid_threshold < RUL80 <= early_threshold\n",
    "        return df_in[(df_in[\"RUL80\"] > mid_thr) & (df_in[\"RUL80\"] <= early_thr)]\n",
    "    elif phase_name == \"late\":\n",
    "        # Keep samples with RUL80 <= mid_thr\n",
    "        return df_in[df_in[\"RUL80\"] <= mid_thr]\n",
    "    else:\n",
    "        # If no phase is specified or phase is None, do not filter\n",
    "        return df_in\n",
    "\n",
    "if phase is not None:\n",
    "    df_train = filter_by_phase(df_train, phase, early_threshold, mid_threshold)\n",
    "    df_val = filter_by_phase(df_val, phase, early_threshold, mid_threshold)\n",
    "    df_test = filter_by_phase(df_test, phase, early_threshold, mid_threshold)\n",
    "\n",
    "# --- 5) Extract sequences (X) and target RUL80 (y) ---\n",
    "history_train = df_train[\"History\"].tolist()\n",
    "history_val = df_val[\"History\"].tolist()\n",
    "history_test = df_test[\"History\"].tolist()\n",
    "\n",
    "y_train = np.array(df_train[\"RUL80\"])\n",
    "y_val = np.array(df_val[\"RUL80\"])\n",
    "y_test = np.array(df_test[\"RUL80\"])\n",
    "\n",
    "# --- 5.1) (OPTIONAL) Log transform the target ---\n",
    "# Make sure RUL is positive. If there are zeros or near-zero values, consider log1p.\n",
    "if log_transform:\n",
    "    # You can use np.log1p if you're worried about zero or negative values:\n",
    "    # y_train, y_val, y_test = np.log1p(y_train), np.log1p(y_val), np.log1p(y_test)\n",
    "    y_train, y_val, y_test = np.log(y_train), np.log(y_val), np.log(y_test)\n",
    "\n",
    "# --- 6) Normalize historical capacity sequences ---\n",
    "# Flatten all histories to fit one MinMaxScaler\n",
    "all_histories = history_train + history_val + history_test\n",
    "if len(all_histories) == 0:\n",
    "    raise ValueError(\n",
    "        f\"No data left after applying phase='{phase}'. \"\n",
    "        \"Try a different threshold or remove the phase filter.\"\n",
    "    )\n",
    "\n",
    "all_histories_flat = np.concatenate(all_histories)\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(all_histories_flat.reshape(-1, 1))\n",
    "\n",
    "history_train_normalized = [\n",
    "    scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_train\n",
    "]\n",
    "history_val_normalized = [\n",
    "    scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_val\n",
    "]\n",
    "history_test_normalized = [\n",
    "    scaler.transform(np.array(h).reshape(-1, 1)).flatten() for h in history_test\n",
    "]\n",
    "\n",
    "# --- 7) Normalize target values (post-log if log_transform=True) ---\n",
    "# We'll still scale by dividing by max from the training set (but note it's now log-scale if log_transform=True).\n",
    "y_max = y_train.max() if len(y_train) > 0 else 1.0  # Avoid divide-by-zero\n",
    "\n",
    "y_train_norm = y_train / y_max\n",
    "y_val_norm = y_val / y_max\n",
    "y_test_norm = y_test / y_max\n",
    "\n",
    "# --- 8) Pad sequences to the maximum sequence length ---\n",
    "max_sequence_length = max(len(h) for h in all_histories)\n",
    "X_train_padded = pad_sequences(\n",
    "    history_train_normalized,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding='post',\n",
    "    dtype='float32'\n",
    ")\n",
    "X_val_padded = pad_sequences(\n",
    "    history_val_normalized,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding='post',\n",
    "    dtype='float32'\n",
    ")\n",
    "X_test_padded = pad_sequences(\n",
    "    history_test_normalized,\n",
    "    maxlen=max_sequence_length,\n",
    "    padding='post',\n",
    "    dtype='float32'\n",
    ")\n",
    "\n",
    "# --- 9) Reshape for LSTM input (samples, time steps, features) ---\n",
    "X_train_lstm = X_train_padded[..., np.newaxis]\n",
    "X_val_lstm = X_val_padded[..., np.newaxis]\n",
    "X_test_lstm = X_test_padded[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show tensorflow-metal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GPU Devices:\", tf.config.list_physical_devices('GPU'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade tensorflow-macos tensorflow-metal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Available devices:\", tf.config.list_physical_devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)  # Should print 2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow==2.4.1 (from versions: 2.13.0rc0, 2.13.0rc1, 2.13.0rc2, 2.13.0, 2.13.1, 2.14.0rc0, 2.14.0rc1, 2.14.0, 2.14.1, 2.15.0rc0, 2.15.0rc1, 2.15.0, 2.15.1, 2.16.0rc0, 2.16.1, 2.16.2, 2.17.0rc0, 2.17.0rc1, 2.17.0, 2.17.1, 2.18.0rc0, 2.18.0rc1, 2.18.0rc2, 2.18.0)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow==2.4.1\u001b[0m\u001b[31m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow==2.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
