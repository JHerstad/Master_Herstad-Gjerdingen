{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TimeSHAP\n",
    "\n",
    "Used on Aachen dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeshap\n",
    "\n",
    "# Import necessary functions and modules from the script\n",
    "from LSTM_Model_Training import (\n",
    "    get_config,\n",
    "    preprocess_aachen_dataset,\n",
    "    build_model,\n",
    "    plot_training_history,\n",
    "    plot_predictions_vs_actual,\n",
    "    plot_residuals,\n",
    "    explain_with_shap,\n",
    "    save_model_structure_and_weights,\n",
    "    load_model_structure_and_weights,\n",
    "    get_unique_model_name\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load pre-trained model...\n",
      "Checking for model files in the following paths:\n",
      "Structure file: Aachen/Models/model_20250131_124416.structure.json\n",
      "Weights file: Aachen/Models/model_20250131_124416.weights.h5\n",
      "Contents of directory 'Aachen/Models':\n",
      "['model_20250127_135003.structure.json', 'model_20250131_124416.weights.h5', 'model_20250127_135003.weights.h5', 'model_20250128_130021.structure.json', 'model_20250128_130021.weights.h5', 'model_20250131_123220.structure.json', 'model_20250131_124416.structure.json', 'model_20250131_123220.weights.h5']\n",
      "Model loaded from Aachen/Models/model_20250131_124416.structure.json and Aachen/Models/model_20250131_124416.weights.h5\n",
      "Pre-trained model loaded successfully.\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 4.8971e-04 - mae: 0.0180\n",
      "\n",
      "Test Loss: 0.0009450705256313086\n",
      "Test MAE: 0.024175753816962242\n",
      "\u001b[1m19/19\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "   Actual RUL80  Predicted RUL80\n",
      "0        1060.0      1029.755249\n",
      "1        1055.0      1029.259155\n",
      "2        1050.0      1026.125732\n",
      "3        1045.0      1021.070496\n",
      "4        1040.0      1014.818420\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load configuration\n",
    "config = get_config()\n",
    "\n",
    "# Step 2: Preprocess the Aachen dataset\n",
    "data_path = \"/Users/sigurdgjerdingen/Student/Master kode/Master_Herstad-Gjerdingen/Aachen/Degradation_Prediction_Dataset_ISEA.mat\"\n",
    "aachen_data = preprocess_aachen_dataset(\n",
    "    data_path,\n",
    "    test_cell_count=3,\n",
    "    random_state=42,\n",
    "    phase=None,\n",
    "    log_transform=False,\n",
    ")\n",
    "\n",
    "# Extract the preprocessed data\n",
    "X_train_lstm = aachen_data[\"X_train\"]\n",
    "X_val_lstm = aachen_data[\"X_val\"]\n",
    "X_test_lstm = aachen_data[\"X_test\"]\n",
    "y_train = aachen_data[\"y_train\"]\n",
    "y_val = aachen_data[\"y_val\"]\n",
    "y_test = aachen_data[\"y_test\"]\n",
    "y_max = aachen_data[\"y_max\"]\n",
    "\n",
    "# Step 3: Define model name for loading or saving\n",
    "model_name = \"Aachen/Models/model_20250131_124416\"\n",
    "#model_name = \"Aachen/Models/model_20250127_135003\"  # Update as needed\n",
    "\n",
    "if model_name is None:\n",
    "    model_name = get_unique_model_name()\n",
    "\n",
    "# Step 4: Load or train the model\n",
    "try:\n",
    "    print(\"Attempting to load pre-trained model...\")\n",
    "    model = load_model_structure_and_weights(model_name)\n",
    "    print(\"Pre-trained model loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"No pre-trained model found. Training a new model...\")\n",
    "\n",
    "    # Build the model\n",
    "    model = build_model(X_train_lstm.shape[1:], config)\n",
    "\n",
    "    # Define callbacks\n",
    "    callbacks = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss', patience=config[\"patience\"], restore_best_weights=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train_lstm, y_train,\n",
    "        validation_data=(X_val_lstm, y_val),\n",
    "        epochs=config[\"epochs\"],\n",
    "        batch_size=config[\"batch_size\"],\n",
    "        verbose=1,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "\n",
    "    # Save the best model after training\n",
    "    save_model_structure_and_weights(model, model_name)\n",
    "    print(f\"New model saved as {model_name}\")\n",
    "\n",
    "    # Plot training history\n",
    "    plot_training_history(history)\n",
    "\n",
    "    # Step 5: Evaluate the model on the test set\n",
    "test_loss, test_mae = model.evaluate(X_test_lstm, y_test, verbose=1)\n",
    "print(f\"\\nTest Loss: {test_loss}\")\n",
    "print(f\"Test MAE: {test_mae}\")\n",
    "\n",
    "# Step 6: Make predictions on the test set\n",
    "y_pred = model.predict(X_test_lstm)\n",
    "\n",
    "# Rescale predictions and test data back to the original range\n",
    "y_pred_rescaled = y_pred.flatten() * y_max\n",
    "y_test_rescaled = y_test * y_max\n",
    "\n",
    "# Compare actual and predicted values\n",
    "results = pd.DataFrame({\n",
    "    \"Actual RUL80\": y_test_rescaled,\n",
    "    \"Predicted RUL80\": y_pred_rescaled\n",
    "})\n",
    "print(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = X_train_lstm  # shape (7190, 272, 1), your dataset\n",
    "X_flat = X.reshape(-1)  # flatten to 1D\n",
    "valid_values = X_flat[X_flat != 0]  # remove zeros\n",
    "X_train_baseline = valid_values.mean()      # or use .median()\n",
    "X_train_baseline = np.array([[X_train_baseline]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Kernel' from 'shap.explainers._kernel' (/opt/miniconda3/envs/D2D_env/lib/python3.11/site-packages/shap/explainers/_kernel.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimeshap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m local_pruning, local_event, local_feat, local_cell_level\n",
      "File \u001b[0;32m/opt/miniconda3/envs/D2D_env/lib/python3.11/site-packages/timeshap/explainer/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#  Copyright 2022 Feedzai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#  See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#  limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpruning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevent_level\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/D2D_env/lib/python3.11/site-packages/timeshap/explainer/kernel/__init__.py:15\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#  Copyright 2022 Feedzai\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#  Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m#  See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m#  limitations under the License.\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtimeshap_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TimeShapKernel\n",
      "File \u001b[0;32m/opt/miniconda3/envs/D2D_env/lib/python3.11/site-packages/timeshap/explainer/kernel/timeshap_kernel.py:53\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_legacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_link, IdentityLink\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_legacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m convert_to_instance, convert_to_model\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexplainers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_kernel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Kernel\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binom\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m issparse\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Kernel' from 'shap.explainers._kernel' (/opt/miniconda3/envs/D2D_env/lib/python3.11/site-packages/shap/explainers/_kernel.py)"
     ]
    }
   ],
   "source": [
    "from timeshap.explainer import local_pruning, local_event, local_feat, local_cell_level\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda x: model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TimeSHAP' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m explainer \u001b[38;5;241m=\u001b[39m TimeSHAP(f)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Explain the first instance in the test set\u001b[39;00m\n\u001b[1;32m      4\u001b[0m explanation \u001b[38;5;241m=\u001b[39m explainer\u001b[38;5;241m.\u001b[39mattribute(X_test_lstm[\u001b[38;5;241m0\u001b[39m:\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Single sequence\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TimeSHAP' is not defined"
     ]
    }
   ],
   "source": [
    "explainer = TimeSHAP(f)\n",
    "\n",
    "# Explain the first instance in the test set\n",
    "explanation = explainer.attribute(X_test_lstm[0:1])  # Single sequence\n",
    "print(explanation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
